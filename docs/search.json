[
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Blog",
    "section": "",
    "text": "MEDS Capstone Project\n\n\n\nCapstone\n\n\nJavaScript\n\n\nPython\n\n\nApplication Development\n\n\nMEDS\n\n\n\nIdentifying Disadvantaged Communities Using Cumulative Environmental Burdens\n\n\n\nHaylee Oyler\n\n\nJun 26, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSo You Want to Win a Kaggle Competition?\n\n\n\nMachine-Learning\n\n\nPython\n\n\nMEDS\n\n\n\nBuilding an Extreme Gradient Boosted Model to Predict Seawater Chemistry\n\n\n\nHaylee Oyler\n\n\nMar 30, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReproducing the Weather\n\n\n\nDEIJ\n\n\nStatistics\n\n\nR\n\n\nMEDS\n\n\n\nSimulating the Consequences of Early-Life Rainfall on Life Outcomes\n\n\n\nHaylee Oyler\n\n\nMar 21, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGender Bias in Academic Publishing\n\n\n\nDEIJ\n\n\nData-Visualization\n\n\nMEDS\n\n\n\nInvestigating the Gender Gap in Academia through Published Research Articles\n\n\n\nHaylee Oyler\n\n\nMar 11, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat You Don’t Know You’re Afraid Of\n\n\n\nEnvironmental-Justice\n\n\nMEDS\n\n\nEthics\n\n\n\nHow Landscapes of Fear Drive Environmental Research\n\n\n\nHaylee Oyler\n\n\nDec 19, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWildfires and Respiratory Health in California\n\n\n\nEnvironmental-Health\n\n\nR\n\n\nStatistics\n\n\nMEDS\n\n\n\nExamining the Effect of Wildfire and AQI on Asthma\n\n\n\nHaylee Oyler\n\n\nDec 13, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThomas Fire AQI and Burn Scar\n\n\n\nEnvironmental-Health\n\n\nPython\n\n\nMEDS\n\n\n\nExploring the Environmental and Health Effects of the 2017 Thomas Fire\n\n\n\nHaylee Oyler\n\n\nDec 7, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Haylee Oyler",
    "section": "",
    "text": "Haylee Oyler\n\n\nInterdisciplinary Environmental Data Scientist\n\n\n\n\n\n  \n    \n\n    \n  \n    \n     LinkedIn\n  \n  \n    \n     GitHub\n  \n  \n    \n     Email\n  \n\n  \n  \n\nMy name is Haylee Oyler (she/they) and I am an interdisciplinary environmental data scientist. In other words, I want to understand environmental problems using approaches, frameworks, and methods across different fields of study. I’ve always been interested in understanding the intersection of people and nature. For me, the most conspicuous path into this nexus was a degree in environmental science from UC Berkeley. I gained experience studying human-environment interactions and it also introduced me to working with data.\n\n\nNow, I am a Master of Environmental Data Science student at UC Santa Barbara. I’m excited about continuing to apply data science tools to study issues of environmental justice and community-based conservation. My approach to solving environmental problems centers first and foremost on people, particularly the participation and experience of historically marginalized groups."
  },
  {
    "objectID": "posts/2025-03-30-kaggle/kaggle.html",
    "href": "posts/2025-03-30-kaggle/kaggle.html",
    "title": "So You Want to Win a Kaggle Competition?",
    "section": "",
    "text": "Are growing in prevalence and efficacy in environmental science. What began in 1949 as an attempt to write a computer program that could play checkers [1] has grown into one of the fastest developing fields today. Machine learning is used to across multiple disciplines—from image recognition models designed to improve the speed of medical diagnoses [2] to the program that filters your spam from your normal inbox [3]—machine learning’s reach has spread far and wide. The study of our natural world and its processes is no exception.\nMachine learning has allowed major advances in the environmental field as well. As data collection techniques only become more refined, frequent, and numerous, we need processing techniques that can match the scale of this data.\nAs part of a final project in Dr. Matteo Robbin’s Machine Learning in Environmental Science, we were tasked with optimizing a machine learning model that would predict dissolved inorganic carbon content (DIC) in a sample of seawater based on a number of other associated characteristics. This was held as part of a kaggle competiton where our evaluation metric was root mean squared error (rmse). The winner was determined based on which model had the smallest rmse on the private leaderboard. Overall, this assesses how accurate each model is at predicting DIC when generalizing to unseen data.\nI will walk through my winning model for this competition: an extreme gradient boosted model (XGB) with bayesian hyperoptimization of parameters.\n\n\n\n\nLet’s go word by word through an extreme gradient boosted model and explain each piece.\n\nBoosted: Boosting is an ensemble method in which multiple weak decision trees are trained sequentially. In simpler terms, you train many “short” decision trees on top of each other and you use the residual error from the previous tree to train the following tree. This allows the model to iteratively optimize performance without a tendency to overfit.\nGradient: Gradient refers to the idea of gradient descent, which is the optimization technique used to minimize the loss function. This starts to get into the math weeds, so I’ll link some resources for those who’d like more detail [4] [5]. But at it’s most conceptual, let’s imagine our parameter space as standing on top of a hill. If we look down, there are numerous different slopes and valleys that mark the terrain between the peak and the bottom of the hill. This “terrain variation” can be thought of as the unique parameter space of our model. Now, imagine I were to drop 100 ping pong balls from the top of this hill and I want to know which ball reached the bottom the fastest. The fastest ping pong boll can be thought of as akin to the gradient of our loss function. That is, gradient descent finds the direction of steepest increase to minimize the loss function.\nExtreme: Now that we have a conceptual understanding of gradient descent, there are many different ways you can set up your how your model finds the most optimum gradient. The term “extreme” comes from the popular xgboost library that is designed to be especially efficient and flexible. There are other types of gradient descent, like batch gradient descent or stochastic gradient descent [6], but XGB is a very common method due to its high performance, built-in regularization, and parallel computing capabilities.\n\n\n\n\nNow that we have our model itself established, let’s talk about how I decided to select parameters for the model with hyperopt. hyperopt is a python library that uses bayesian optimization to find the best parameters. It has three main parts: an objective function, a domain space, and a search algorithm [7].\n\nBayesian optimization: This is another area that gets into the weeds [8], but it can be thought of as a probabilistic, model-based technique to minimize a function. It’s quicker than a random search of parameters because it uses the posterior distribution to establish which parameter spaces are most worth exploring. In this way, the future parameter combinations are informed by the previous ones.\n\nObjective function: This is the function we want our bayesian model to minimize. This function will take our input domain space and output the validation metric (in our case, RMSE). The objective function for this model is the XGB model discussed above. We want to minimize our error given that exact model construction, so naturally, we optimize our hyperparameters based on that model.\nDomain space: The set of hyperparameters and their input values over which we want to search.\nOptimization algorithm: The optimization algorithm used in this model is Tree of Parzen Estimators (TPE). This is where the Bayesian optimization discussed above actually happens.\n\n\nAt the end of the hyperopt process, we have a set of parameters that returns the smallest RMSE. Then, we can train our model on the best parameters.\n\n\n\nThe data used in this model comes courtesy of Dr. Erin Satterthwaite at the California Cooperative Oceanic Fisheries Investigations(CalCOFI)\n\n\n\nLat_Dec: Observed Latitude in decimal degrees\nLon_Dec: Observed Longitude in decimal degrees\nNO2uM: Micromoles Nitrite per liter of seawater\nNO3uM: Micromoles Nitrate per liter of seawater\nNH3uM: Micromoles Ammonia per liter of seawater\nR_TEMP: Reported (Potential) Temperature in degrees Celsius\nR_Depth: Reported Depth (from pressure) in meters\nR_Sal: Reported Salinity (from Specific Volume Anomoly, M³/Kg)\nR_DYNHT: Reported Dynamic Height in units of dynamic meters (work per unit mass)\nR_Nuts: Reported Ammonium concentration\nR_Oxy_micromol.Kg: Reported Oxygen micromoles/kilogram\nPO4uM: Micromoles Phosphate per liter of seawater\nSiO3uM: Micromoles Silicate per liter of seawater\nTA1.x: Total Alkalinity micromoles per kilogram solution\nSalinity1: Salinity (Practical Salinity Scale 1978)\nTemperature_degC: Water temperature in degrees Celsius\nDIC: Dissolved Inorganic Carbon micromoles per kilogram solution\n\n\n\n\n\n\n\n# Load basic libraries\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport statistics as stats\nimport time\n\n# XGB libraries\nfrom sklearn.model_selection import train_test_split,RandomizedSearchCV, cross_val_score, KFold\nimport xgboost as xgb\nfrom hyperopt import hp, fmin, tpe, Trials, STATUS_OK\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom scipy.stats import uniform, randint\nfrom sklearn.preprocessing import StandardScaler\n\n\n\n\n# Import data\ntrain_df = pd.read_csv(\"~/MEDS/website/haylee360.github.io/posts/2025-03-30-kaggle/data/train.csv\")\ntest_df = pd.read_csv(\"~/MEDS/website/haylee360.github.io/posts/2025-03-30-kaggle/data/test.csv\")\n\n# Fix column name error\ntest_df = test_df.rename(columns={'TA1':'TA1.x'})\n\n# Remove NA column from training data\ntrain_df = train_df.drop(columns='Unnamed: 12')\n\n\n# Get a feel for feature summary stats\ntrain_df.describe()\n\n\n\n\n\n\n\n\nid\nLat_Dec\nLon_Dec\nNO2uM\nNO3uM\nNH3uM\nR_TEMP\nR_Depth\nR_Sal\nR_DYNHT\nR_Nuts\nR_Oxy_micromol.Kg\nPO4uM\nSiO3uM\nTA1.x\nSalinity1\nTemperature_degC\nDIC\n\n\n\n\ncount\n1454.000000\n1454.000000\n1454.000000\n1454.000000\n1454.000000\n1454.000000\n1454.000000\n1454.000000\n1454.000000\n1454.000000\n1454.000000\n1454.000000\n1454.000000\n1454.000000\n1454.000000\n1454.000000\n1454.000000\n1454.000000\n\n\nmean\n727.500000\n33.271315\n-120.216359\n0.062252\n18.885812\n0.085062\n10.882772\n193.451857\n224.527854\n0.374726\n0.085062\n146.507682\n1.644869\n29.171437\n2256.054409\n33.764094\n10.901307\n2150.468820\n\n\nstd\n419.877958\n0.891261\n1.719873\n0.284517\n14.414059\n0.190922\n3.702193\n347.486135\n88.427864\n0.365226\n0.190922\n92.421033\n1.024450\n28.628682\n35.215125\n0.398409\n3.684964\n113.163645\n\n\nmin\n1.000000\n30.417500\n-124.000670\n0.000000\n0.000000\n0.000000\n1.250000\n1.000000\n44.900000\n0.003000\n0.000000\n0.000000\n0.170000\n0.000000\n2181.570000\n32.840000\n1.520000\n1948.850000\n\n\n25%\n364.250000\n32.654580\n-121.844853\n0.000000\n1.877500\n0.000000\n8.185000\n30.000000\n149.475000\n0.107000\n0.000000\n59.170572\n0.490000\n3.585000\n2230.032500\n33.417000\n8.215000\n2025.818652\n\n\n50%\n727.500000\n33.420670\n-120.025080\n0.014000\n22.600000\n0.010000\n9.900000\n101.000000\n202.000000\n0.293500\n0.010000\n136.267250\n1.820000\n24.150000\n2244.020000\n33.746800\n9.910000\n2166.630000\n\n\n75%\n1090.750000\n34.150520\n-118.630000\n0.050000\n31.500000\n0.090000\n13.667500\n252.000000\n299.075000\n0.577750\n0.090000\n244.636050\n2.560000\n45.675000\n2279.175000\n34.149450\n13.667500\n2252.657500\n\n\nmax\n1454.000000\n34.663330\n-117.308600\n8.190000\n42.000000\n2.750000\n22.750000\n3595.000000\n485.900000\n3.226000\n2.750000\n332.347700\n4.280000\n175.200000\n2433.710000\n34.676000\n22.750000\n2367.800000\n\n\n\n\n\n\n\n\n# Check NAs\ntrain_df.isna().sum()\n\nid                   0\nLat_Dec              0\nLon_Dec              0\nNO2uM                0\nNO3uM                0\nNH3uM                0\nR_TEMP               0\nR_Depth              0\nR_Sal                0\nR_DYNHT              0\nR_Nuts               0\nR_Oxy_micromol.Kg    0\nPO4uM                0\nSiO3uM               0\nTA1.x                0\nSalinity1            0\nTemperature_degC     0\nDIC                  0\ndtype: int64\n\n\n\n# Visualize feature relationships\nsns.pairplot(train_df, y_vars=['DIC'], x_vars= train_df.columns[1:-1], diag_kind='kde')\n\n\n\n\n\n\n\n\n\n\n\nThe relationships look mostly linear, but we’re working with a lot of features. I figured gradient boosting would be a good approach.\n\n# Assign features\nX = train_df.drop(columns=['id', 'DIC'], axis=1)\ny = train_df['DIC']\nX_test = test_df.drop(columns=['id'], axis=1) \n\n# Scale the data\nscaler = StandardScaler()\nX_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n# For predictions later on...\nX_test_scaled = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)\n\n\n\n\n\n# Set up kfold cross validation\nkf = KFold(n_splits=5, shuffle=True, random_state=808)\n\n# Define objective function to minimize\ndef objective(params):\n    model = XGBRegressor(\n        n_estimators=int(params[\"n_estimators\"]),\n        learning_rate=params[\"learning_rate\"],\n        max_depth=int(params[\"max_depth\"]),\n        min_child_weight=params[\"min_child_weight\"],\n        subsample=params[\"subsample\"],\n        colsample_bytree=params[\"colsample_bytree\"],\n        gamma=params[\"gamma\"],\n        reg_alpha=params[\"reg_alpha\"],\n        reg_lambda=params[\"reg_lambda\"],\n        random_state=808\n    )\n    \n    # Perform cross-validation\n    scores = -cross_val_score(model, X_scaled, y, cv=kf, scoring='neg_root_mean_squared_error', n_jobs=-1)\n\n    # Average RMSE across folds\n    rmse = np.mean(scores)\n\n    return {'loss': rmse, 'status': STATUS_OK}\n\n\n\n\n\n# Create hyperparameter space\nspace = {\n    \"n_estimators\": hp.quniform(\"n_estimators\", 100, 1200, 10),\n    \"learning_rate\": hp.uniform(\"learning_rate\", 0.005, 0.3),\n    \"max_depth\": hp.quniform(\"max_depth\", 3, 20, 1),\n    \"min_child_weight\": hp.uniform(\"min_child_weight\", 1, 10),\n    \"subsample\": hp.uniform(\"subsample\", 0.5, 1.0),\n    \"colsample_bytree\": hp.uniform(\"colsample_bytree\", 0.5, 1.0),\n    \"gamma\": hp.uniform(\"gamma\", 0, 10),  \n    \"reg_alpha\": hp.uniform(\"reg_alpha\", 0, 1),  \n    \"reg_lambda\": hp.uniform(\"reg_lambda\", 0, 1),  \n}\n\n\n\n\n\n# Run hyperopt\ntrials = Trials()\nbest_params = fmin(\n    fn=objective, \n    space=space,      \n    algo=tpe.suggest, \n    max_evals=200,\n    trials=trials,       \n    rstate=np.random.default_rng(808)  \n)\n\n\n\n\nNow that we’ve optimized all of our relevant parameters, we can train our XGB model. We use **best_params to unpack the best parameters from before and initialize an XGBRegressor model.\n\n# Convert int hyperparameters to fix type error\nbest_params[\"n_estimators\"] = int(best_params[\"n_estimators\"])\nbest_params[\"max_depth\"] = int(best_params[\"max_depth\"])\n\n# Initialize best hyperopt model\nxgb_hyper = XGBRegressor(**best_params, eval_metric='rmse', random_state=808)\n\n# Fit model\nxgb_hyper.fit(X_scaled, y)\n\n# Predict on test data\ny_pred_hyper = xgb_hyper.predict(X_test_scaled)\n\n\n# Get feature importance\nfeat_imp_hyper = pd.DataFrame({'Feature': X_scaled.columns, 'Importance': xgb_hyper.feature_importances_})\n\n# Sort by importance\nfeat_imp_hyper = feat_imp_hyper.sort_values(by=\"Importance\", ascending=False)\nfeat_imp_hyper\n\n\n\n\n\n\n\n\nFeature\nImportance\n\n\n\n\nSiO3uM\n0.439688\n\n\nPO4uM\n0.375247\n\n\nR_Oxy_micromol.Kg\n0.099113\n\n\nR_Sal\n0.032475\n\n\nNO3uM\n0.017117\n\n\nTA1.x\n0.015256\n\n\nSalinity1\n0.012926\n\n\nR_Depth\n0.004667\n\n\nNO2uM\n0.000781\n\n\nTemperature_degC\n0.000693\n\n\nR_TEMP\n0.000607\n\n\nR_DYNHT\n0.000389\n\n\nNH3uM\n0.000282\n\n\nLat_Dec\n0.000281\n\n\nLon_Dec\n0.000257\n\n\nR_Nuts\n0.000222\n\n\n\n\n\nNow that we’ve generated our predictions on the test data, all we need to do is add those to their associated ID’s in the test_df and export to csv for submission to the competition.\n\n# Add DIC to test dataset\ntest_df['DIC'] = y_pred_hyper\nsubmission = test_df[['id', 'DIC']]\nsubmission.head()\n\n\n\n\n\n\n\n\n\n\nid\nDIC\n\n\n\n\n0\n1455\n2170.5910\n\n\n1\n1456\n2194.9880\n\n\n2\n1457\n2326.0432\n\n\n3\n1458\n1991.1729\n\n\n4\n1459\n2147.3965\n\n\n\n\n\n\n\n\n# Export for submission\nsubmission.to_csv('submission.csv', index=False)\n\nAnd just like that, you can have a competition-winning machine learning model! A very big thanks to Professor Robbins for his guidance in this course, Dr. Satterthwaite for her wonderful guest lecture, and Annie Adams for her assistance all quarter."
  },
  {
    "objectID": "posts/2025-03-30-kaggle/kaggle.html#the-model-breakdown",
    "href": "posts/2025-03-30-kaggle/kaggle.html#the-model-breakdown",
    "title": "So You Want to Win a Kaggle Competition?",
    "section": "",
    "text": "Let’s go word by word through an extreme gradient boosted model and explain each piece.\n\nBoosted: Boosting is an ensemble method in which multiple weak decision trees are trained sequentially. In simpler terms, you train many “short” decision trees on top of each other and you use the residual error from the previous tree to train the following tree. This allows the model to iteratively optimize performance without a tendency to overfit.\nGradient: Gradient refers to the idea of gradient descent, which is the optimization technique used to minimize the loss function. This starts to get into the math weeds, so I’ll link some resources for those who’d like more detail [4] [5]. But at it’s most conceptual, let’s imagine our parameter space as standing on top of a hill. If we look down, there are numerous different slopes and valleys that mark the terrain between the peak and the bottom of the hill. This “terrain variation” can be thought of as the unique parameter space of our model. Now, imagine I were to drop 100 ping pong balls from the top of this hill and I want to know which ball reached the bottom the fastest. The fastest ping pong boll can be thought of as akin to the gradient of our loss function. That is, gradient descent finds the direction of steepest increase to minimize the loss function.\nExtreme: Now that we have a conceptual understanding of gradient descent, there are many different ways you can set up your how your model finds the most optimum gradient. The term “extreme” comes from the popular xgboost library that is designed to be especially efficient and flexible. There are other types of gradient descent, like batch gradient descent or stochastic gradient descent [6], but XGB is a very common method due to its high performance, built-in regularization, and parallel computing capabilities.\n\n\n\n\nNow that we have our model itself established, let’s talk about how I decided to select parameters for the model with hyperopt. hyperopt is a python library that uses bayesian optimization to find the best parameters. It has three main parts: an objective function, a domain space, and a search algorithm [7].\n\nBayesian optimization: This is another area that gets into the weeds [8], but it can be thought of as a probabilistic, model-based technique to minimize a function. It’s quicker than a random search of parameters because it uses the posterior distribution to establish which parameter spaces are most worth exploring. In this way, the future parameter combinations are informed by the previous ones.\n\nObjective function: This is the function we want our bayesian model to minimize. This function will take our input domain space and output the validation metric (in our case, RMSE). The objective function for this model is the XGB model discussed above. We want to minimize our error given that exact model construction, so naturally, we optimize our hyperparameters based on that model.\nDomain space: The set of hyperparameters and their input values over which we want to search.\nOptimization algorithm: The optimization algorithm used in this model is Tree of Parzen Estimators (TPE). This is where the Bayesian optimization discussed above actually happens.\n\n\nAt the end of the hyperopt process, we have a set of parameters that returns the smallest RMSE. Then, we can train our model on the best parameters.\n\n\n\nThe data used in this model comes courtesy of Dr. Erin Satterthwaite at the California Cooperative Oceanic Fisheries Investigations(CalCOFI)\n\n\n\nLat_Dec: Observed Latitude in decimal degrees\nLon_Dec: Observed Longitude in decimal degrees\nNO2uM: Micromoles Nitrite per liter of seawater\nNO3uM: Micromoles Nitrate per liter of seawater\nNH3uM: Micromoles Ammonia per liter of seawater\nR_TEMP: Reported (Potential) Temperature in degrees Celsius\nR_Depth: Reported Depth (from pressure) in meters\nR_Sal: Reported Salinity (from Specific Volume Anomoly, M³/Kg)\nR_DYNHT: Reported Dynamic Height in units of dynamic meters (work per unit mass)\nR_Nuts: Reported Ammonium concentration\nR_Oxy_micromol.Kg: Reported Oxygen micromoles/kilogram\nPO4uM: Micromoles Phosphate per liter of seawater\nSiO3uM: Micromoles Silicate per liter of seawater\nTA1.x: Total Alkalinity micromoles per kilogram solution\nSalinity1: Salinity (Practical Salinity Scale 1978)\nTemperature_degC: Water temperature in degrees Celsius\nDIC: Dissolved Inorganic Carbon micromoles per kilogram solution"
  },
  {
    "objectID": "posts/2025-03-30-kaggle/kaggle.html#the-coding-breakdown",
    "href": "posts/2025-03-30-kaggle/kaggle.html#the-coding-breakdown",
    "title": "So You Want to Win a Kaggle Competition?",
    "section": "",
    "text": "# Load basic libraries\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport statistics as stats\nimport time\n\n# XGB libraries\nfrom sklearn.model_selection import train_test_split,RandomizedSearchCV, cross_val_score, KFold\nimport xgboost as xgb\nfrom hyperopt import hp, fmin, tpe, Trials, STATUS_OK\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom scipy.stats import uniform, randint\nfrom sklearn.preprocessing import StandardScaler\n\n\n\n\n# Import data\ntrain_df = pd.read_csv(\"~/MEDS/website/haylee360.github.io/posts/2025-03-30-kaggle/data/train.csv\")\ntest_df = pd.read_csv(\"~/MEDS/website/haylee360.github.io/posts/2025-03-30-kaggle/data/test.csv\")\n\n# Fix column name error\ntest_df = test_df.rename(columns={'TA1':'TA1.x'})\n\n# Remove NA column from training data\ntrain_df = train_df.drop(columns='Unnamed: 12')\n\n\n# Get a feel for feature summary stats\ntrain_df.describe()\n\n\n\n\n\n\n\n\nid\nLat_Dec\nLon_Dec\nNO2uM\nNO3uM\nNH3uM\nR_TEMP\nR_Depth\nR_Sal\nR_DYNHT\nR_Nuts\nR_Oxy_micromol.Kg\nPO4uM\nSiO3uM\nTA1.x\nSalinity1\nTemperature_degC\nDIC\n\n\n\n\ncount\n1454.000000\n1454.000000\n1454.000000\n1454.000000\n1454.000000\n1454.000000\n1454.000000\n1454.000000\n1454.000000\n1454.000000\n1454.000000\n1454.000000\n1454.000000\n1454.000000\n1454.000000\n1454.000000\n1454.000000\n1454.000000\n\n\nmean\n727.500000\n33.271315\n-120.216359\n0.062252\n18.885812\n0.085062\n10.882772\n193.451857\n224.527854\n0.374726\n0.085062\n146.507682\n1.644869\n29.171437\n2256.054409\n33.764094\n10.901307\n2150.468820\n\n\nstd\n419.877958\n0.891261\n1.719873\n0.284517\n14.414059\n0.190922\n3.702193\n347.486135\n88.427864\n0.365226\n0.190922\n92.421033\n1.024450\n28.628682\n35.215125\n0.398409\n3.684964\n113.163645\n\n\nmin\n1.000000\n30.417500\n-124.000670\n0.000000\n0.000000\n0.000000\n1.250000\n1.000000\n44.900000\n0.003000\n0.000000\n0.000000\n0.170000\n0.000000\n2181.570000\n32.840000\n1.520000\n1948.850000\n\n\n25%\n364.250000\n32.654580\n-121.844853\n0.000000\n1.877500\n0.000000\n8.185000\n30.000000\n149.475000\n0.107000\n0.000000\n59.170572\n0.490000\n3.585000\n2230.032500\n33.417000\n8.215000\n2025.818652\n\n\n50%\n727.500000\n33.420670\n-120.025080\n0.014000\n22.600000\n0.010000\n9.900000\n101.000000\n202.000000\n0.293500\n0.010000\n136.267250\n1.820000\n24.150000\n2244.020000\n33.746800\n9.910000\n2166.630000\n\n\n75%\n1090.750000\n34.150520\n-118.630000\n0.050000\n31.500000\n0.090000\n13.667500\n252.000000\n299.075000\n0.577750\n0.090000\n244.636050\n2.560000\n45.675000\n2279.175000\n34.149450\n13.667500\n2252.657500\n\n\nmax\n1454.000000\n34.663330\n-117.308600\n8.190000\n42.000000\n2.750000\n22.750000\n3595.000000\n485.900000\n3.226000\n2.750000\n332.347700\n4.280000\n175.200000\n2433.710000\n34.676000\n22.750000\n2367.800000\n\n\n\n\n\n\n\n\n# Check NAs\ntrain_df.isna().sum()\n\nid                   0\nLat_Dec              0\nLon_Dec              0\nNO2uM                0\nNO3uM                0\nNH3uM                0\nR_TEMP               0\nR_Depth              0\nR_Sal                0\nR_DYNHT              0\nR_Nuts               0\nR_Oxy_micromol.Kg    0\nPO4uM                0\nSiO3uM               0\nTA1.x                0\nSalinity1            0\nTemperature_degC     0\nDIC                  0\ndtype: int64\n\n\n\n# Visualize feature relationships\nsns.pairplot(train_df, y_vars=['DIC'], x_vars= train_df.columns[1:-1], diag_kind='kde')\n\n\n\n\n\n\n\n\n\n\n\nThe relationships look mostly linear, but we’re working with a lot of features. I figured gradient boosting would be a good approach.\n\n# Assign features\nX = train_df.drop(columns=['id', 'DIC'], axis=1)\ny = train_df['DIC']\nX_test = test_df.drop(columns=['id'], axis=1) \n\n# Scale the data\nscaler = StandardScaler()\nX_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n# For predictions later on...\nX_test_scaled = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)\n\n\n\n\n\n# Set up kfold cross validation\nkf = KFold(n_splits=5, shuffle=True, random_state=808)\n\n# Define objective function to minimize\ndef objective(params):\n    model = XGBRegressor(\n        n_estimators=int(params[\"n_estimators\"]),\n        learning_rate=params[\"learning_rate\"],\n        max_depth=int(params[\"max_depth\"]),\n        min_child_weight=params[\"min_child_weight\"],\n        subsample=params[\"subsample\"],\n        colsample_bytree=params[\"colsample_bytree\"],\n        gamma=params[\"gamma\"],\n        reg_alpha=params[\"reg_alpha\"],\n        reg_lambda=params[\"reg_lambda\"],\n        random_state=808\n    )\n    \n    # Perform cross-validation\n    scores = -cross_val_score(model, X_scaled, y, cv=kf, scoring='neg_root_mean_squared_error', n_jobs=-1)\n\n    # Average RMSE across folds\n    rmse = np.mean(scores)\n\n    return {'loss': rmse, 'status': STATUS_OK}\n\n\n\n\n\n# Create hyperparameter space\nspace = {\n    \"n_estimators\": hp.quniform(\"n_estimators\", 100, 1200, 10),\n    \"learning_rate\": hp.uniform(\"learning_rate\", 0.005, 0.3),\n    \"max_depth\": hp.quniform(\"max_depth\", 3, 20, 1),\n    \"min_child_weight\": hp.uniform(\"min_child_weight\", 1, 10),\n    \"subsample\": hp.uniform(\"subsample\", 0.5, 1.0),\n    \"colsample_bytree\": hp.uniform(\"colsample_bytree\", 0.5, 1.0),\n    \"gamma\": hp.uniform(\"gamma\", 0, 10),  \n    \"reg_alpha\": hp.uniform(\"reg_alpha\", 0, 1),  \n    \"reg_lambda\": hp.uniform(\"reg_lambda\", 0, 1),  \n}\n\n\n\n\n\n# Run hyperopt\ntrials = Trials()\nbest_params = fmin(\n    fn=objective, \n    space=space,      \n    algo=tpe.suggest, \n    max_evals=200,\n    trials=trials,       \n    rstate=np.random.default_rng(808)  \n)\n\n\n\n\nNow that we’ve optimized all of our relevant parameters, we can train our XGB model. We use **best_params to unpack the best parameters from before and initialize an XGBRegressor model.\n\n# Convert int hyperparameters to fix type error\nbest_params[\"n_estimators\"] = int(best_params[\"n_estimators\"])\nbest_params[\"max_depth\"] = int(best_params[\"max_depth\"])\n\n# Initialize best hyperopt model\nxgb_hyper = XGBRegressor(**best_params, eval_metric='rmse', random_state=808)\n\n# Fit model\nxgb_hyper.fit(X_scaled, y)\n\n# Predict on test data\ny_pred_hyper = xgb_hyper.predict(X_test_scaled)\n\n\n# Get feature importance\nfeat_imp_hyper = pd.DataFrame({'Feature': X_scaled.columns, 'Importance': xgb_hyper.feature_importances_})\n\n# Sort by importance\nfeat_imp_hyper = feat_imp_hyper.sort_values(by=\"Importance\", ascending=False)\nfeat_imp_hyper\n\n\n\n\n\n\n\n\nFeature\nImportance\n\n\n\n\nSiO3uM\n0.439688\n\n\nPO4uM\n0.375247\n\n\nR_Oxy_micromol.Kg\n0.099113\n\n\nR_Sal\n0.032475\n\n\nNO3uM\n0.017117\n\n\nTA1.x\n0.015256\n\n\nSalinity1\n0.012926\n\n\nR_Depth\n0.004667\n\n\nNO2uM\n0.000781\n\n\nTemperature_degC\n0.000693\n\n\nR_TEMP\n0.000607\n\n\nR_DYNHT\n0.000389\n\n\nNH3uM\n0.000282\n\n\nLat_Dec\n0.000281\n\n\nLon_Dec\n0.000257\n\n\nR_Nuts\n0.000222\n\n\n\n\n\nNow that we’ve generated our predictions on the test data, all we need to do is add those to their associated ID’s in the test_df and export to csv for submission to the competition.\n\n# Add DIC to test dataset\ntest_df['DIC'] = y_pred_hyper\nsubmission = test_df[['id', 'DIC']]\nsubmission.head()\n\n\n\n\n\n\n\n\n\n\nid\nDIC\n\n\n\n\n0\n1455\n2170.5910\n\n\n1\n1456\n2194.9880\n\n\n2\n1457\n2326.0432\n\n\n3\n1458\n1991.1729\n\n\n4\n1459\n2147.3965\n\n\n\n\n\n\n\n\n# Export for submission\nsubmission.to_csv('submission.csv', index=False)\n\nAnd just like that, you can have a competition-winning machine learning model! A very big thanks to Professor Robbins for his guidance in this course, Dr. Satterthwaite for her wonderful guest lecture, and Annie Adams for her assistance all quarter."
  },
  {
    "objectID": "posts/2024-12-19-landscapes-of-fear/landscapes-fear.html",
    "href": "posts/2024-12-19-landscapes-of-fear/landscapes-fear.html",
    "title": "What You Don’t Know You’re Afraid Of",
    "section": "",
    "text": "The snap of the twig outside your tent in the dead of night, the dizzying swoop of your stomach when you look over a high ledge. Your palms sweat, your heart races. Fear is designed to keep us alive–a mechanism we’ve evolved to assess threats to our safety.\nEcologists, in particular, think about fear in many ways. They’ve coined the term “landscapes of fear,” which is a framework to describe how prey perceive and respond to the risk of predation[1]). For example, to a deer, the best food in the forest might be in the center of the open meadow, but the deer is also aware that being in the open meadow makes them an obvious target to the mountain lion. While ecologists consider how fear affects the decision-making of species, new scholarship has posed a different perspective: how does fear affect the decision-making of the ecologists themselves?\nA recent publication by Gabriel Gadsden, Nigel Golden, and Nyeema Harris repurposes this term into what they call social-ecological landscapes of fear, that is, a landscape of fear derived from the impact of negative human histories across space[2]. They call for the inclusion of negative human histories as important drivers of landscape change. This means considering the legacy of “mortality, extermination, subordination, and dispossession related to ethically controversial conditions, including overpolicing, redlining, racial or ethnic violence, corrupt governments, low social capital, and segregation” across space and time[2]. They also posit that failing to address negative human histories generates a place-based bias that affects where and how scientists conduct research.\n\n\n\nFigure 1. Conceptual graphic of the problematic framing of landscapes in conventional conservation and environmental scholarship where landscapes are dichotomized between pristine ecologically relevant and degraded landscapes. (a) Green space (b) Nature trails (c) Charismatic megafauna (d) Suburban neighborhood (e) Nuisance wildlife (f) Cityscape (g) War and conflict (h) Forced removal (i) Redlining (j) Land conversion (k) Pollution (l) Industry\n\n\nThis fear-based dismissal of landscapes manifests in how scholars decide what are considered intact environments. One example of social-ecological landscapes of fear causing place-based bias is the dismissal and erasure of land due to pollution. The case study I will use to discuss these ideas is uranium mining in the Navajo Nation."
  },
  {
    "objectID": "posts/2024-12-19-landscapes-of-fear/landscapes-fear.html#fear.-weve-all-felt-it.",
    "href": "posts/2024-12-19-landscapes-of-fear/landscapes-fear.html#fear.-weve-all-felt-it.",
    "title": "What You Don’t Know You’re Afraid Of",
    "section": "",
    "text": "The snap of the twig outside your tent in the dead of night, the dizzying swoop of your stomach when you look over a high ledge. Your palms sweat, your heart races. Fear is designed to keep us alive–a mechanism we’ve evolved to assess threats to our safety.\nEcologists, in particular, think about fear in many ways. They’ve coined the term “landscapes of fear,” which is a framework to describe how prey perceive and respond to the risk of predation[1]). For example, to a deer, the best food in the forest might be in the center of the open meadow, but the deer is also aware that being in the open meadow makes them an obvious target to the mountain lion. While ecologists consider how fear affects the decision-making of species, new scholarship has posed a different perspective: how does fear affect the decision-making of the ecologists themselves?\nA recent publication by Gabriel Gadsden, Nigel Golden, and Nyeema Harris repurposes this term into what they call social-ecological landscapes of fear, that is, a landscape of fear derived from the impact of negative human histories across space[2]. They call for the inclusion of negative human histories as important drivers of landscape change. This means considering the legacy of “mortality, extermination, subordination, and dispossession related to ethically controversial conditions, including overpolicing, redlining, racial or ethnic violence, corrupt governments, low social capital, and segregation” across space and time[2]. They also posit that failing to address negative human histories generates a place-based bias that affects where and how scientists conduct research.\n\n\n\nFigure 1. Conceptual graphic of the problematic framing of landscapes in conventional conservation and environmental scholarship where landscapes are dichotomized between pristine ecologically relevant and degraded landscapes. (a) Green space (b) Nature trails (c) Charismatic megafauna (d) Suburban neighborhood (e) Nuisance wildlife (f) Cityscape (g) War and conflict (h) Forced removal (i) Redlining (j) Land conversion (k) Pollution (l) Industry\n\n\nThis fear-based dismissal of landscapes manifests in how scholars decide what are considered intact environments. One example of social-ecological landscapes of fear causing place-based bias is the dismissal and erasure of land due to pollution. The case study I will use to discuss these ideas is uranium mining in the Navajo Nation."
  },
  {
    "objectID": "posts/2024-12-19-landscapes-of-fear/landscapes-fear.html#case-study-uranium-mining-on-navajo-land",
    "href": "posts/2024-12-19-landscapes-of-fear/landscapes-fear.html#case-study-uranium-mining-on-navajo-land",
    "title": "What You Don’t Know You’re Afraid Of",
    "section": "Case Study: Uranium Mining on Navajo Land",
    "text": "Case Study: Uranium Mining on Navajo Land\nIn the 1940s, uranium mining began in the U.S. when the Atomic Energy Commission was seeking a local source of uranium to supply nuclear power plants[3]. Several areas in the Southwestern United States were selected as suitable sites, including land on the Navajo Nation. For the next 40 years, Navajo people and other community members would work in radioactive mines where they were exposed to harmful chemicals known to cause lung cancer. However, thanks to the tireless work of Navajo activists and public health officials, these mines were eventually shut down. The Radiation Exposure Compensation Act (RECA) was passed in 1990, in which the U.S. government acknowledged its responsibility for the mistreatment of miners and provided financial compensation for illness.\nWhile RECA was a huge step forward, remediation and cleanup of radioactive waste is still an active problem in the region today[4]. Remediation is a primary focus of research in the area, but the scientific literature around the mines in the Navajo Nation still predominantly centers on narratives of deficit and dysfunction. Connecting this legacy to social-ecological landscapes of fear, I argue that the negative histories around radioactive pollution have created an underlying place-based bias in the environmental literature of the region. The perception of the landscape as degraded has led to a dismissal of the region as important for conservation efforts.\nWhile data bias can come from many sources, in this case, the place-based bias originates from the underlying frameworks researchers use for analysis. One of the first steps in experimental design is selecting a study site and population. If your selection process does not consider landscapes across a spectrum of negative human histories equally, then you are generating a place-based selection bias in your methods."
  },
  {
    "objectID": "posts/2024-12-19-landscapes-of-fear/landscapes-fear.html#the-impact-of-place-based-bias",
    "href": "posts/2024-12-19-landscapes-of-fear/landscapes-fear.html#the-impact-of-place-based-bias",
    "title": "What You Don’t Know You’re Afraid Of",
    "section": "The Impact of Place-Based Bias",
    "text": "The Impact of Place-Based Bias\nWhen researchers do not perceive landscapes equally, then they do not study those landscapes equally. Place-based bias in environmental scholarship means conservation efforts are overvaluing places seen as “pristine” while undervaluing areas seen as “degraded.” For example, Chapin discusses how large conservation organizations have historically seen places where Indigenous people reside as unfit for conservation because they prioritize “their economic wellbeing over preservation of natural resources[5]. Indigenous nations want the autonomy to manage their lands and resources as they see fit. Yet because their decision-making on that land now marks it as less than pristine from the dominant global north perspective, it becomes understudied and undervalued. Their mere presence on the land adds it to the subconscious social-ecological landscape of fear that underpins dominant science research.\nIn the case of the Navajo people, the landscape of fear around the legacy of the mines has meant a lack of consideration of the ecological value of the region, such as its biodiversity and ecosystem services. When the dominant narrative of a place becomes entrenched in its complicated past, then it is easy to reduce that place down to its worst qualities. This reductionism occurs not only to the physical space itself but also to the people who occupy that space.\nPlace-based bias can also be readily found across urban and suburban landscapes. Community science data, for example, often display a strong bias toward areas the participants find enjoyable, aesthetically pleasing, or safe. This leads to an oversurveying of urban green spaces and an undersurveying of dense cityscapes[6]. Negative human histories of redlining, segregation, and discrimination shape what urban areas are desirable to conduct community science, and what urban areas are erased from the narrative. These factors contribute to the social-ecological landscape of fear that guides environmental scholarship.\nPlace-based bias from landscapes of fear is a subtle, pervasive bias across environmental literature. As researchers, we must acknowledge that true objectivity does not exist. There are always unknown unknowns that impact our decision-making. By calling attention to how fear colors our perception of landscapes, we can turn this place-based bias into a known unknown. While recognition of the impact of negative human histories on research is the first step, there are also solutions to mitigate the effects of landscapes of fear."
  },
  {
    "objectID": "posts/2024-12-19-landscapes-of-fear/landscapes-fear.html#solutions",
    "href": "posts/2024-12-19-landscapes-of-fear/landscapes-fear.html#solutions",
    "title": "What You Don’t Know You’re Afraid Of",
    "section": "Solutions",
    "text": "Solutions\nPlace-based bias generated from landscapes of fear can be mitigated by two broad paths: recognizing negative histories, and the collaborative cocreation of knowledge with community members. Recognizing histories starts with an acknowledgment of past harm. In environmental scholarship, a common example is land acknowledgments. Situating your research in the larger colonial context is a simple and effective first step toward shedding light on potential negative human histories associated with your work.\nCommunity collaboration and cocreation go beyond acknowledgement and seek to create meaningful connections with the people who live in places of interest. It also involves scrutinizing underlying power structures and cocreating new methods that seek to rectify an imbalance of power. The rise of indigenous data sovereignty and data methods is an example of a framework that gives authority back to historically marginalized groups. This means bringing capacity for research to Indigenous communities, such as data collection, analysis, and reporting methods, and advocating for Indigenous research methodologies to be prioritized and respected in academic settings[7]. This also looks like shifting the narrative around the values that underpin data collection and use. For example, the CARE principles for Indigenous data governance center on collective benefit, authority to control, responsibility, and ethics[8].\nAs researchers from a settler colonial positionality, recognizing the data sovereignty of Indigenous peoples and prioritizing CARE principles in our underlying framework of data science is the first step toward cocreating knowledge with Indigenous communities. However, it is important to mention that these things only matter if you have been invited in. The legacy of colonialism is fraught with violence. Approaching collaboration from a place of humility, introspection, and respect is necessary to ensure collaborative efforts don’t reproduce harm.\n\n\n\nFigure 2. A guide designed as a catalyst to overcome social–ecological landscapes of fear in environmental scholarship (Gadsden et al. 2022)\n\n\nThese principles of acknowledgement and cocreation apply to all community groups that exist in spaces impacted by negative human histories. As environmental scientists invested in conserving the world around us–both people and place–we must listen and understand the stories of negative human histories across landscapes. Only by having the full perspective can we make informed decisions about how we study natural spaces. Often, we fear things simply because we do not understand them. But knowledge disrupts fear, and as researchers, we must respect and value the knowledge of those who live in the landscapes we study."
  },
  {
    "objectID": "posts/2025-06-26-capstone/capstone.html",
    "href": "posts/2025-06-26-capstone/capstone.html",
    "title": "MEDS Capstone Project",
    "section": "",
    "text": "The hallmark of the Master of Environmental Data Science program at the Bren School of Environmental Science and Management is the capstone project. This is a 6-month project where groups of 3-4 master’s students tackle real-world environmental problems with data-driven solutions. I had the honor of being apart of the Justice40 team. This included Josephine Cardelle, Kat Le, Kimberlee Wong and myself, advised by Dr. Jayajit Chakraborty.\nOur goal? Expand the Climate and Economic Justice Screening Tool to incorporate data on cumulative environmental burdens. Before I dive into how we accomplished this goal, let’s get into some background.\n\n\nJustice40 was a federal initiative created by the Biden Administration in early 2021 [1]. It’s aim was to direct 40% of funding from climate-related federal investments to disadvantaged communities. Justice40 was signifcant in its creation because of its whole of government approach. This meant multiple federal agencies working together to fulfill the initiative’s goal and was a huge step toward creating an orchestrated federal response to ameliorate environmental harm.\nThe creation of Justice40 also meant that the federal government needed a formal definition of what it meant to be disadvantaged and a way to identify where these disadvantaged communities were. This was the purpose of the Climate and Economic Justice Screening Tool (CEJST)\n\n\n\nCEJST was created by the White House Council on Environmental Quality as a way to identify communities that were eligible for Justice40 funds. It did this by using data from over 30 different sources to create a broad metric of a communities environmental and economic burden. It looked at various characteristics of a census tract, a small region inside a county, and assigned that tract as disadvantaged (DAC) or non-disadvantaged (non-DAC).\n\n\nIn the original CEJST framework, DACs were defined by two criteria: a low-income threshold and a high vulnerability threshold.\nLow-income: 65th national percentile or higher of people in households where the income was less than or equal to twice the federal poverty level.\nHigh vulnerability: Characterized by meeting one of eight burden thresholds shown below. Each threshold has between 2-5 associated indicators that comprise that burden and define how you meet the threshold.\nIMAGE\nFor example, if your census tract was in the 90th national percentile or higher for asthma rates, then you would meet the health burden threshold. This meant that compared to all other census tracts in the U.S., your communities’ asthma rates would be in the top 10%.\nMost of the indicators use a percentile scale, but some are a “yes” or “no” category. For example, historically disadvantaged communities are assigned using redlining data as a “yes” or “no.”\nQualifying as low-income in combination with at least one burden threshold meant that your community would be defined as disadvantaged in the eyes of Justice40.\n\n\n\n\nWhile the CEJST framework was incredibly innovative supported thousands of underserved communities, it also had some drawbacks. One of the largest being that there is no measure of the cumulative effect of high vulnerability. This meant that a census tract with only one burden threshold exceeded would be assigned the same status as a census tract that had 6 thresholds exceeded.\nOur project aims to improve the framework of the CEJST tool to incorporate a more comprehensive understanding of disadvantaged communities\nWe did this with two main approaches: a cumulative analysis and a hot spot analysis.\n\n\nOur cumulative analysis looked at the total number of thresholds exceeded by a community. This included burdens exceeded and indicators exceeded (the sub-category of a burden). While this approach has many underlying assumptions, our intent here was not to provide a true… but rather to give a simple, easy to understand perspective and which areas are more highly burdened than others.\nThis method pulls from established practices for evaluating cumulative impacts in geospatial mapping tools.\nEach census tract receives:\n\nA burden score from 0-8\nAn indicator score from 0-31\n\nIt is important to note that while there are 31 indicators used in the analysis, no census tract exceeds more than 18 total indicators.\nDIAGRAM\n\n\n\nOur hot spot analysis provides a similar examination to our cumulative analysis with more statistical rigor. This analysis uses a spatial statistic called Getis Ord Gi*(often called gi-star)\nDIAGRAM\nThis method finds census tracts where the total burdens or indicators are significantly higher (“hot spots”) or lower (“cold spots”) than the national average.\nKey steps include:\nCalculating local sums of thresholds exceeded for each census tract and its neighbors\nComparing local sums to the global sum minus the local values. This is done by standardizing the sums to a z-score, also known as the standardized Gi* value.\nIn addition to the z-score, the calculation generates a p-value that is interpreted into hot spots or cold spots. The p-values here are slightly different because they are interpreted with a positive or negative sign. The direction is important to note whether an area is significantly hot, or significantly cold.\n\nHigh positives: Hot spots\nHigh negatives: Cold spots"
  },
  {
    "objectID": "posts/2025-06-26-capstone/capstone.html#the-justice-40-initiative",
    "href": "posts/2025-06-26-capstone/capstone.html#the-justice-40-initiative",
    "title": "MEDS Capstone Project",
    "section": "",
    "text": "Justice40 was a federal initiative created by the Biden Administration in early 2021 [1]. It’s aim was to direct 40% of funding from climate-related federal investments to disadvantaged communities. Justice40 was signifcant in its creation because of its whole of government approach. This meant multiple federal agencies working together to fulfill the initiative’s goal and was a huge step toward creating an orchestrated federal response to ameliorate environmental harm.\nThe creation of Justice40 also meant that the federal government needed a formal definition of what it meant to be disadvantaged and a way to identify where these disadvantaged communities were. This was the purpose of the Climate and Economic Justice Screening Tool (CEJST)"
  },
  {
    "objectID": "posts/2025-06-26-capstone/capstone.html#cejst",
    "href": "posts/2025-06-26-capstone/capstone.html#cejst",
    "title": "MEDS Capstone Project",
    "section": "",
    "text": "CEJST was created by the White House Council on Environmental Quality as a way to identify communities that were eligible for Justice40 funds. It did this by using data from over 30 different sources to create a broad metric of a communities environmental and economic burden. It looked at various characteristics of a census tract, a small region inside a county, and assigned that tract as disadvantaged (DAC) or non-disadvantaged (non-DAC).\n\n\nIn the original CEJST framework, DACs were defined by two criteria: a low-income threshold and a high vulnerability threshold.\nLow-income: 65th national percentile or higher of people in households where the income was less than or equal to twice the federal poverty level.\nHigh vulnerability: Characterized by meeting one of eight burden thresholds shown below. Each threshold has between 2-5 associated indicators that comprise that burden and define how you meet the threshold.\nIMAGE\nFor example, if your census tract was in the 90th national percentile or higher for asthma rates, then you would meet the health burden threshold. This meant that compared to all other census tracts in the U.S., your communities’ asthma rates would be in the top 10%.\nMost of the indicators use a percentile scale, but some are a “yes” or “no” category. For example, historically disadvantaged communities are assigned using redlining data as a “yes” or “no.”\nQualifying as low-income in combination with at least one burden threshold meant that your community would be defined as disadvantaged in the eyes of Justice40."
  },
  {
    "objectID": "posts/2025-06-26-capstone/capstone.html#expanding-disadvantaged-status",
    "href": "posts/2025-06-26-capstone/capstone.html#expanding-disadvantaged-status",
    "title": "MEDS Capstone Project",
    "section": "",
    "text": "While the CEJST framework was incredibly innovative supported thousands of underserved communities, it also had some drawbacks. One of the largest being that there is no measure of the cumulative effect of high vulnerability. This meant that a census tract with only one burden threshold exceeded would be assigned the same status as a census tract that had 6 thresholds exceeded.\nOur project aims to improve the framework of the CEJST tool to incorporate a more comprehensive understanding of disadvantaged communities\nWe did this with two main approaches: a cumulative analysis and a hot spot analysis.\n\n\nOur cumulative analysis looked at the total number of thresholds exceeded by a community. This included burdens exceeded and indicators exceeded (the sub-category of a burden). While this approach has many underlying assumptions, our intent here was not to provide a true… but rather to give a simple, easy to understand perspective and which areas are more highly burdened than others.\nThis method pulls from established practices for evaluating cumulative impacts in geospatial mapping tools.\nEach census tract receives:\n\nA burden score from 0-8\nAn indicator score from 0-31\n\nIt is important to note that while there are 31 indicators used in the analysis, no census tract exceeds more than 18 total indicators.\nDIAGRAM\n\n\n\nOur hot spot analysis provides a similar examination to our cumulative analysis with more statistical rigor. This analysis uses a spatial statistic called Getis Ord Gi*(often called gi-star)\nDIAGRAM\nThis method finds census tracts where the total burdens or indicators are significantly higher (“hot spots”) or lower (“cold spots”) than the national average.\nKey steps include:\nCalculating local sums of thresholds exceeded for each census tract and its neighbors\nComparing local sums to the global sum minus the local values. This is done by standardizing the sums to a z-score, also known as the standardized Gi* value.\nIn addition to the z-score, the calculation generates a p-value that is interpreted into hot spots or cold spots. The p-values here are slightly different because they are interpreted with a positive or negative sign. The direction is important to note whether an area is significantly hot, or significantly cold.\n\nHigh positives: Hot spots\nHigh negatives: Cold spots"
  },
  {
    "objectID": "posts/2025-03-11-gender-academia/gender-academia.html",
    "href": "posts/2025-03-11-gender-academia/gender-academia.html",
    "title": "Gender Bias in Academic Publishing",
    "section": "",
    "text": "The World Economic Forum defines the gender gap as “the difference between women and men as reflected in social, political, intellectual, cultural, or economic attainments or attitudes”[1]. In everyday life, the gender gap is often discussed in terms of a difference in pay. You may have heard the statistic “72 cents to a dollar” to describe how much the average woman is paid compared to a man for the same work[2]. Another place the gender gap is visible is academia.\nWhile gender bias is present in a number of ways in academic settings, one specific way this manifests is as a gender gap in published research articles. For students pursuing secondary education in masters or PhD programs, the phrase “publish or perish” is often tossed around the labs of many research universities[3]. This refers to the pressure that young researchers feel to make a name for themselves in their field of study. Publications hold weight, status, and value—especially if you can land a publication in a high-impact journal. And if you can’t publish? Then suddenly your job prospects suddenly aren’t looking as good.\nIn 2020, the publisher Elsevier conducted a global analysis of persistent gender bias in research publications. It was centered in the European union and 15 additional countries across 26 fields of study[4]. To further investigate the relationship between gender and academic publishing, I used this data to create an infographic displaying key trends in gender bias across time, space, and discipline.\nSpecifically, I wanted to answer the questions:\nIs there a gender gap in academic publishing and if so, what does it look like?\n\nWhat is the gender gap across different academic disciplines?\nHow has the gender gap changed over time?\nWhat does the gender gap look like from country to country?\n\nThe data used in this infographic can be found here. Additionally, this data was part of a larger report by Elsevier. The goal of this infographic was to communicate clearly and effectively the state of the gender gap in published research articles. Additionally, I hope to raise awareness about continued inequity in academia. I do this by employing a number of strategies that I discuss below.\n\n\n\nThe first plot I visualized for this data was the dumbbell plot of authors by gender and academic discipline, shown in Figure 3. Dumbbell plots are very commonly used when comparing disparities between two groups and they’re a hallmark when showing gender-related gaps. Originally, I had ordered the dumbbells from most male authors to least, but when I converted the absolute values to percentages and instead sorted by the difference between genders, the dumbbell plot takes on this lovely DNA-esque shape. While the figure size I went with in the end somewhat distorts the DNA image, I was fascinated by this form and knew I wanted to center this in the final infographic.\nThe next plot that came relatively quickly were the two pie charts. In the final infographic, I mask the charts to fit the shape of a “journal”, but you can see the orignal visualization in Figure 2. The dates for these two plots might feel arbitrary, but that’s just because those were the only two time periods available to work with in the data. I would love to have more complete data and do a time series analysis of gender and publishings over time.\nThe last visualization that came together was the map. At first, I had done a simple dodged bar chart of total authors by country. This got the job done, but was rather boring in combination with the other charts. So, at the recommendation of a classmate, I decided to turn it into a choropleth map instead, as shown in Figure 1. This visualization is still somewhat unsatisfying because there was only data for 26 countries, so there are large swaths of missing gray tiles on the map, but overall I am happy with how it came out in comparison with the original bar chart. Plus, maps add a lot of visual flavor that regular plots and charts can sometimes lack.\n\n\n\nAs far as the general design of the infographic, a lot of choices were constrained by a lack of space and a lot of information to communicate. It’s amazing how much you can pack into just three visualizations, and trying to communicate all of that clearly in a document that’s reasonably sized was challening. Thus, my final infographic does have a lot of text on the page in addition to the visualizations themselves. I felt added notes of context and quotes from figures of authority where important to add here. I wanted to do this topic justice and felt contextual explanations were the easiest and most straightforward way to make sure the message of the infographic was delivered accurately and clearly.\nAt first, I chose a vertical orientation for the page setup, but I felt this made it hard to fit the rather large map and dumbbell plot without feeling too cramped. So, I pivoted to a horizontal infographic. This allowed me to give both large visualizations most of the space on the page while also leaving breathing room between elements. This is important to help your reader not feel too overwhelmed by text or images on the page.\n\n\nColor is something I went back and forth on for awhile. Color should be chosen thoughtfully when visualizing gender because you do not want to reinforce outdated stereotypes, but you also don’t want intentionally obfuscate the message of the plot. For example, neither using pink for women  and blue for men, nor pink for men and blue for women is a good idea. The first option leans into gender stereotypes; the second option confuses readers becauses it presents the opposite of what they expect.\nThe palette you see on the infographic comes courtesy of Sam Shanny-Csik that was used in a gender-related visualization. I liked this palette because it’s monochromatic with purple being the main hue. This single hue and varied saturation means its accessible to people with color-blindness. It also adds a sense of uniformity to the entire presentation to be shown one base color the entire time.\nHowever, this also meant I had a difficult time with contrast for the lighter color used to denote “women.” This text color is harder to read on the light purple background. I did my best to maximize legibility and also consider switching palettes entirely, but ultimately I decided to stick with these colors because of the benefits discussed above.\nAdditionally, I personally thought choosing to talk about gender using purple as a combination of pink and blue was a nice choice. There are other colors (like yellow or green) that are truly free from gender association in the U.S., but I wanted to lean into the purple, even if it is still traditionally a “feminine” color. The infographic is about raising awareness to increase womens’ voice in academic publishing, so if some people think it looks feminine, well, that’s alright with me.\n\n\n\nThere is a lot of text on this infographic. I believe this choice was both deliberate and necessary given the topic. The map and dumbbell chart both have annotations that draw attention to important features of each resepctive visualization. These notes help guide the reader to the most important takeaways for that particular chart. For the map specifically, I chose to remove any title or subtitle and instead let the legend title communicate the data shown on the map. The map felt cluttered with an additional title right below the block of background text at the top, so shifting it to a concise legend title instead felt better. All other text was kept as brief and clear as possible to leave space for the visualizations themselves.\nI decided to stick with one font for the entire infographic: Lexend. I looked for a font that might pair well with Lexend as a title font and even briefly considered switching to my ipad to try some handwritten annotations, but ultimately I stuck with just one font. This choice was made along a similar logic as sticking with purple as the main hue. I wanted visual simplicity and clarity across the whole infographic, so I felt sticking with one uniform font helped contribute to that. Additionally, I endeavored to make the font as large as possible given the spatial context. As someone with poor eyesight myself, I am always frustrated by small text on screens or in presentations. Thus, legibility was very important to me when looking at the infographic as a whole.\nLastly, theme elements for all the plots where uniformly assigned and kept as minimal as possible. Most of the customization happened in Affinity Designer afterwards, so I wanted as clean of a template as possible to build on when exporting my visualizations. Some notable theme elements I did keep were the major grid lines for the dumbbell plot. There are a lot of individual data points on that plot and I wanted to make it easy for the reader to see exactly what percentages they represented.\n\n\n\n\nOne vital aspect of this visualization to me was to ensure the binary nature of gender that was represented in the data was properly contextualized. The process of using an algorithm to assign gender based off name alone is fraught with many layers of complexity and bias that I could spend multiple blog posts talking about. That being said, I still wanted to use this very imperfect data to talk about I subject I believe is important. Ultimately, the best way for me to discuss identities outside of “man” and “woman” came in the form of a call-out note near the bottom of the infographic. This felt like the least I could do to call attention to the fact that non-gender conforming individuals do crucially important work in academic spaces and deserve to be considered equally in conversations around gender in academia.\nBeyond the biases present in the data themselves. I also wanted to include voices of a few women in academia working hard to push for equality. The first quote is from Dr. Miyoko O. Watanabe, the deputy executive director and director of the office for diversity and inclusion at the Japan Science and Technology Agency. I appreciated how Dr. Watanabe emphasized the importance of recognizing intersectionality in discussions of diversity. The second quote is from Charlina Vitcheva (MA, MS) who is the acting director-general of the Joint Research Centre and European Commission. I felt this quote was a fitting way to both acknowledge past progress while also encouraging future advancements. Both of these quotes came from Elsevier’s report that was created from the dataset I use in this infographic.\nThese quotes were small ways to drive home the primary message of the infographic, which is twofold: to explore the gender gap in academic publshing, and to raise awareness for inequity in academia.\n\n\n\n\nAll the code for the visualizations can be viewed by clicking the dropdown arrows. Each arrow is accompanied by a short description of what that code chunk is for.\n\n\nLoad libraries, data, and settings\nlibrary(tidyverse)\nlibrary(here)\nlibrary(janitor)\nlibrary(readxl)\nlibrary(patchwork)\nlibrary(showtext)\nlibrary(glue)\nlibrary(ggtext)\nlibrary(scales)\nlibrary(sf)\nlibrary(tmap)\nlibrary(grid)\nlibrary(gridtext)\n\n# Read in data\nauthor_stats = read_xlsx(here(\"data\", \"authors.xlsx\"), sheet = 1) %&gt;%\n  clean_names() %&gt;% \n  mutate(gender = str_to_title(gender))\n\n# Read in countries geometries\ncountry_bounds &lt;- read_sf(here(\"data\", \"countries\", \"world-administrative-boundaries.shp\")) %&gt;% \n  select(iso3, name, geometry)\n\n# Enable showtext\nshowtext_auto()\n\n# Ensure showtext is used\nshowtext_opts(dpi = 300)\n\n# Import font\nfont_add_google(name = \"Lexend\", family = \"lexend\")\n\n\n\n\n\n\nData wrangling\n# Calculate summary stats grouped by field\ncountries &lt;- author_stats %&gt;% \n  group_by(country, gender) %&gt;% \n  summarise(country_gender = sum(authors)) %&gt;% \n  mutate(total_authors = sum(country_gender),\n         percent_country = country_gender/total_authors)\n\n\n# pivot wider to add columns of the number of authors by field by men or women\ncountries_wide &lt;- countries %&gt;%\n  pivot_wider(\n    id_cols = c(country, total_authors),\n    names_from = gender,\n    values_from = c(country_gender, percent_country),\n    names_prefix = \"\"\n  ) %&gt;%\n  group_by(country) %&gt;%\n  mutate(total_authors = first(total_authors),\n         gender_gap = country_gender_Men - country_gender_Women,\n         percent_gender_gap = percent_country_Men - percent_country_Women,\n         gender_ratio = country_gender_Men/country_gender_Women) %&gt;%\n  ungroup() \n\n# Join countries geometries\ncountries_wide &lt;- left_join(countries_wide, country_bounds, by = join_by(country== iso3))\n\n# Reassert that it's a sf data frame\ncountries_wide &lt;- countries_wide %&gt;% \n  st_as_sf() %&gt;% \n  drop_na() %&gt;% \n  filter(!name %in% c(\"Azores Islands\")) # remove extra country \n\n# Decided to bin the gender ratio to make the visualization easier\ncountries_new &lt;- countries_wide %&gt;% \n  mutate(gender_ratio = round(gender_ratio / 0.5) * 0.5)\n\n\n\n\nMapping code\n# Map of average publications by country\nmap &lt;- tm_shape(countries_new) +\n  tm_fill(\"gender_ratio\",\n          palette = c(\"#F2C5FD\", \"#EC9BFC\", \"#D67BE6\", \n                       \"#A93CBA\", \"#8C2AA7\", \"#6A1E99\"),\n          breaks = c(0, 1, 1.5, 2, 2.5, 7),\n          labels = c('1 : 1', '1 : 1.5', '1 : 2', '1 : 2.5', '1 : 7'),\n          title = \"Ratio of Female : Male\\nAuthors\") +\n  tm_shape(country_bounds) +\n  tm_borders(col = \"black\", lwd = 0.8) +\n  tm_layout(main.title = \"Difference in Average Publications Across Countries\\nBetween Men and Women\",\n            main.title.size = 1.8, \n            main.title.position = \"center\",\n            main.title.fontface = \"bold\",\n            bg.color = \"transparent\",\n            inner.margins = c(0, 0, 0, 0),  # Adjust margins (top, right, bottom, left)\n            legend.position = c(0, 0),\n            legend.text.size = 1.1,\n            legend.title.size = 1.1,\n            fontfamily = \"lexend\",\n            frame = FALSE)\n\nmap\n\n\n\n\n\n\n\n\nFigure 1: The gender publishing gap by country\n\n\n\n\n\n\n\n\nData wrangling\n# Filter data into the two different time periods\n\n# Older data\nauthor_old &lt;- author_stats %&gt;% \n  filter(period == \"1999-2003\")\n\n# Generate some summary stats\nold_sum &lt;- author_old %&gt;% \n  group_by(gender) %&gt;% \n  summarise(gender_authors = sum(authors)) %&gt;% \n  ungroup() %&gt;% \n  mutate(total_authors = sum(gender_authors), \n         percent_pub = gender_authors/total_authors)\n\n# More recent data\nauthor_new &lt;- author_stats %&gt;% \n  filter(period == \"2014-2018\")\n\n# Generate some summary stats\nnew_sum &lt;- author_new %&gt;% \n  group_by(gender) %&gt;% \n  summarise(gender_authors = sum(authors)) %&gt;% \n  ungroup() %&gt;% \n  mutate(total_authors = sum(gender_authors), \n         percent_pub = gender_authors/total_authors)\n\n\n\n\nPie chart code\n# Pie chart for the older years\nold_pie &lt;- ggplot(old_sum, aes(x = \"\", y = percent_pub, fill = gender)) +\n  geom_bar(stat = \"identity\", width = 1) +\n  coord_polar(\"y\", start = 0) +\n  geom_text(aes(label = paste0(gender, \"\\n\", scales::percent(percent_pub, accuracy = 1)), family = \"lexend\"),\n            position = position_stack(vjust = 0.5),\n            color = \"white\", size = 6) +\n  scale_fill_manual(values = c(\"Women\" = \"#ec9bfc\", \"Men\" = \"#6A1E99\")) + \n  theme_void() +  \n  labs(subtitle = \"1993-2003\",\n       fill = \"\") +\n  theme(\n    text = element_text(family = \"lexend\"),\n    legend.text = element_text(family = \"lexend\"),\n    plot.subtitle = element_text(family = \"lexend\", hjust = 0.5),\n    legend.position = \"none\"\n  )\n\n# Pie chart for the more recent years\nnew_pie &lt;- ggplot(new_sum, aes(x = \"\", y = percent_pub, fill = gender)) +\n  geom_bar(stat = \"identity\", width = 1) + \n  coord_polar(\"y\", start = 0) +\n  geom_text(aes(label = paste0(gender, \"\\n\", scales::percent(percent_pub, accuracy = 1)), family = \"lexend\"),\n            position = position_stack(vjust = 0.5),\n            color = \"white\", size = 6) +\n  scale_fill_manual(values = c(\"Women\" = \"#ec9bfc\", \"Men\" = \"#6A1E99\")) + \n  theme_void() +  \n  labs(subtitle = \"2014-2018\",\n       fill = \"\") +\n  theme(\n    text = element_text(family = \"lexend\"),\n    legend.text = element_text(family = \"lexend\"),\n    plot.subtitle = element_text(family = \"lexend\", hjust = 0.5),\n    legend.position = \"none\"\n  )\n\n# Stick the two figures together\npatchwork &lt;- old_pie + new_pie\npatch_final &lt;- patchwork + plot_annotation(\n  title = \"Women have increased their share of academic publications\\nby 10% from 1993 to 2018\",\n  caption = \"Source: Elsevier Data Repository | Graphic: Haylee Oyler\"\n) &\n  theme( \n    text = element_text(family = \"lexend\"),\n    plot.title = element_text(face = \"bold\", size = 20),\n    plot.subtitle = element_text(size = 16),\n    plot.background = element_rect(fill = \"white\", color = NA),\n    legend.text = element_text(size=16, face = \"bold\"),\n    plot.caption = element_text(size=10),\n    plot.background = element_rect(fill = \"transparent\", color = NA),\n    panel.background = element_rect(fill = \"transparent\", color = NA),\n    legend.background = element_rect(fill = \"transparent\", color = NA), \n    legend.key = element_rect(fill = \"transparent\", color = NA) \n    )\npatch_final\n\n\n\n\n\n\n\n\nFigure 2: The gender publishing gap over time\n\n\n\n\n\n\n\n\nData wrangling\n# Calculate summary stats grouped by field\nfields &lt;- author_stats %&gt;% \n  group_by(subject_area_or_subfield, gender) %&gt;% \n  summarise(field_gender = sum(authors)) %&gt;% \n  mutate(total_authors = sum(field_gender),\n         percent_field = field_gender/total_authors)\n\n\n# pivot wider to add columns of the number of authors by field by men or women\nfields_wide &lt;- fields %&gt;%\n  pivot_wider(\n    id_cols = c(subject_area_or_subfield, total_authors),\n    names_from = gender,\n    values_from = c(field_gender, percent_field),\n    names_prefix = \"\"\n  ) %&gt;%\n  group_by(subject_area_or_subfield) %&gt;%\n  mutate(total_authors = first(total_authors),\n         gender_gap = field_gender_Men - field_gender_Women,\n         percent_gender_gap = percent_field_Men - percent_field_Women) %&gt;%\n  ungroup() %&gt;% \n  filter(subject_area_or_subfield != \"ALL\")\n\n# Reorder data by the gender gap from high to low\nfields_wide &lt;- fields_wide %&gt;% \n  mutate(subject_area_or_subfield = fct_reorder(.f = subject_area_or_subfield, \n                                                .x = percent_gender_gap))\n\n\n\n\nDumbbell plot code\n# Create color-coded plot title\ndumbbell_title &lt;- glue::glue(\"&lt;span style='color:#6A1E99;'&gt;**Men**&lt;/span&gt; \n                       Publish More than \n                       &lt;span style='color:#ec9bfc;'&gt;**Women**&lt;/span&gt;\n                       Across\\nMost Academic Fields\")\n\n\n# dumbbell plot\ndumbbell_plot &lt;- ggplot(fields_wide) +\n  geom_linerange(aes(y = subject_area_or_subfield,\n                     xmin = percent_field_Women, \n                     xmax = percent_field_Men)) +\n  geom_point(aes(x = percent_field_Women, \n                 y = subject_area_or_subfield, \n                 color = \"Women\"), size = 3.5) +\n  geom_point(aes(x = percent_field_Men, \n                 y = subject_area_or_subfield, \n                 color = \"Men\"),\n             size = 3.5) +\n  geom_vline(xintercept = .5, linetype = \"dashed\", color = \"gray40\") +\n  scale_x_continuous(breaks = seq(0, 1, by = 0.1),\n                     labels = scales::percent_format(scale = 100)) +  \n  scale_color_manual(values = c(\"Women\" = \"#ec9bfc\", \"Men\" = \"#6A1E99\")) + \n  labs(title = dumbbell_title, \n      x = \"Percentage of Total Publications\",\n      y = \"\") +\n  theme_minimal(base_size = 20)  + \n    theme(\n    text = element_text(family = \"lexend\"),\n    plot.title = ggtext::element_textbox(face = \"bold\", \n                                         margin = margin(t = 0,r = 0, \n                                                         b = 0, l = -130),\n                                         padding = margin(t = 0, r = 0, \n                                                          b = 15, l = 0)),\n    axis.title =  ggtext::element_textbox(size = rel(1.1),\n                                          color = \"black\",\n                                          padding = margin(t = 10, r = 0, \n                                                           b = 0, l = 0)),\n    axis.text.x = element_text(size=rel(1), face = \"bold\"),\n    axis.text.y = element_text(size=rel(0.9), color = \"black\"),\n    plot.caption = element_text(size=rel(0.6)),\n    panel.grid.minor.x = element_blank(),\n    legend.position=\"none\"\n    )\n\ndumbbell_plot\n\n\n\n\n\n\n\n\nFigure 3: The gender publishing gap across disciplines"
  },
  {
    "objectID": "posts/2025-03-11-gender-academia/gender-academia.html#why-these-plots",
    "href": "posts/2025-03-11-gender-academia/gender-academia.html#why-these-plots",
    "title": "Gender Bias in Academic Publishing",
    "section": "",
    "text": "The first plot I visualized for this data was the dumbbell plot of authors by gender and academic discipline, shown in Figure 3. Dumbbell plots are very commonly used when comparing disparities between two groups and they’re a hallmark when showing gender-related gaps. Originally, I had ordered the dumbbells from most male authors to least, but when I converted the absolute values to percentages and instead sorted by the difference between genders, the dumbbell plot takes on this lovely DNA-esque shape. While the figure size I went with in the end somewhat distorts the DNA image, I was fascinated by this form and knew I wanted to center this in the final infographic.\nThe next plot that came relatively quickly were the two pie charts. In the final infographic, I mask the charts to fit the shape of a “journal”, but you can see the orignal visualization in Figure 2. The dates for these two plots might feel arbitrary, but that’s just because those were the only two time periods available to work with in the data. I would love to have more complete data and do a time series analysis of gender and publishings over time.\nThe last visualization that came together was the map. At first, I had done a simple dodged bar chart of total authors by country. This got the job done, but was rather boring in combination with the other charts. So, at the recommendation of a classmate, I decided to turn it into a choropleth map instead, as shown in Figure 1. This visualization is still somewhat unsatisfying because there was only data for 26 countries, so there are large swaths of missing gray tiles on the map, but overall I am happy with how it came out in comparison with the original bar chart. Plus, maps add a lot of visual flavor that regular plots and charts can sometimes lack."
  },
  {
    "objectID": "posts/2025-03-11-gender-academia/gender-academia.html#general-design",
    "href": "posts/2025-03-11-gender-academia/gender-academia.html#general-design",
    "title": "Gender Bias in Academic Publishing",
    "section": "",
    "text": "As far as the general design of the infographic, a lot of choices were constrained by a lack of space and a lot of information to communicate. It’s amazing how much you can pack into just three visualizations, and trying to communicate all of that clearly in a document that’s reasonably sized was challening. Thus, my final infographic does have a lot of text on the page in addition to the visualizations themselves. I felt added notes of context and quotes from figures of authority where important to add here. I wanted to do this topic justice and felt contextual explanations were the easiest and most straightforward way to make sure the message of the infographic was delivered accurately and clearly.\nAt first, I chose a vertical orientation for the page setup, but I felt this made it hard to fit the rather large map and dumbbell plot without feeling too cramped. So, I pivoted to a horizontal infographic. This allowed me to give both large visualizations most of the space on the page while also leaving breathing room between elements. This is important to help your reader not feel too overwhelmed by text or images on the page.\n\n\nColor is something I went back and forth on for awhile. Color should be chosen thoughtfully when visualizing gender because you do not want to reinforce outdated stereotypes, but you also don’t want intentionally obfuscate the message of the plot. For example, neither using pink for women  and blue for men, nor pink for men and blue for women is a good idea. The first option leans into gender stereotypes; the second option confuses readers becauses it presents the opposite of what they expect.\nThe palette you see on the infographic comes courtesy of Sam Shanny-Csik that was used in a gender-related visualization. I liked this palette because it’s monochromatic with purple being the main hue. This single hue and varied saturation means its accessible to people with color-blindness. It also adds a sense of uniformity to the entire presentation to be shown one base color the entire time.\nHowever, this also meant I had a difficult time with contrast for the lighter color used to denote “women.” This text color is harder to read on the light purple background. I did my best to maximize legibility and also consider switching palettes entirely, but ultimately I decided to stick with these colors because of the benefits discussed above.\nAdditionally, I personally thought choosing to talk about gender using purple as a combination of pink and blue was a nice choice. There are other colors (like yellow or green) that are truly free from gender association in the U.S., but I wanted to lean into the purple, even if it is still traditionally a “feminine” color. The infographic is about raising awareness to increase womens’ voice in academic publishing, so if some people think it looks feminine, well, that’s alright with me.\n\n\n\nThere is a lot of text on this infographic. I believe this choice was both deliberate and necessary given the topic. The map and dumbbell chart both have annotations that draw attention to important features of each resepctive visualization. These notes help guide the reader to the most important takeaways for that particular chart. For the map specifically, I chose to remove any title or subtitle and instead let the legend title communicate the data shown on the map. The map felt cluttered with an additional title right below the block of background text at the top, so shifting it to a concise legend title instead felt better. All other text was kept as brief and clear as possible to leave space for the visualizations themselves.\nI decided to stick with one font for the entire infographic: Lexend. I looked for a font that might pair well with Lexend as a title font and even briefly considered switching to my ipad to try some handwritten annotations, but ultimately I stuck with just one font. This choice was made along a similar logic as sticking with purple as the main hue. I wanted visual simplicity and clarity across the whole infographic, so I felt sticking with one uniform font helped contribute to that. Additionally, I endeavored to make the font as large as possible given the spatial context. As someone with poor eyesight myself, I am always frustrated by small text on screens or in presentations. Thus, legibility was very important to me when looking at the infographic as a whole.\nLastly, theme elements for all the plots where uniformly assigned and kept as minimal as possible. Most of the customization happened in Affinity Designer afterwards, so I wanted as clean of a template as possible to build on when exporting my visualizations. Some notable theme elements I did keep were the major grid lines for the dumbbell plot. There are a lot of individual data points on that plot and I wanted to make it easy for the reader to see exactly what percentages they represented."
  },
  {
    "objectID": "posts/2025-03-11-gender-academia/gender-academia.html#context-message-and-sensitivity",
    "href": "posts/2025-03-11-gender-academia/gender-academia.html#context-message-and-sensitivity",
    "title": "Gender Bias in Academic Publishing",
    "section": "",
    "text": "One vital aspect of this visualization to me was to ensure the binary nature of gender that was represented in the data was properly contextualized. The process of using an algorithm to assign gender based off name alone is fraught with many layers of complexity and bias that I could spend multiple blog posts talking about. That being said, I still wanted to use this very imperfect data to talk about I subject I believe is important. Ultimately, the best way for me to discuss identities outside of “man” and “woman” came in the form of a call-out note near the bottom of the infographic. This felt like the least I could do to call attention to the fact that non-gender conforming individuals do crucially important work in academic spaces and deserve to be considered equally in conversations around gender in academia.\nBeyond the biases present in the data themselves. I also wanted to include voices of a few women in academia working hard to push for equality. The first quote is from Dr. Miyoko O. Watanabe, the deputy executive director and director of the office for diversity and inclusion at the Japan Science and Technology Agency. I appreciated how Dr. Watanabe emphasized the importance of recognizing intersectionality in discussions of diversity. The second quote is from Charlina Vitcheva (MA, MS) who is the acting director-general of the Joint Research Centre and European Commission. I felt this quote was a fitting way to both acknowledge past progress while also encouraging future advancements. Both of these quotes came from Elsevier’s report that was created from the dataset I use in this infographic.\nThese quotes were small ways to drive home the primary message of the infographic, which is twofold: to explore the gender gap in academic publshing, and to raise awareness for inequity in academia."
  },
  {
    "objectID": "posts/2025-03-11-gender-academia/gender-academia.html#the-coding-breakdown",
    "href": "posts/2025-03-11-gender-academia/gender-academia.html#the-coding-breakdown",
    "title": "Gender Bias in Academic Publishing",
    "section": "",
    "text": "All the code for the visualizations can be viewed by clicking the dropdown arrows. Each arrow is accompanied by a short description of what that code chunk is for.\n\n\nLoad libraries, data, and settings\nlibrary(tidyverse)\nlibrary(here)\nlibrary(janitor)\nlibrary(readxl)\nlibrary(patchwork)\nlibrary(showtext)\nlibrary(glue)\nlibrary(ggtext)\nlibrary(scales)\nlibrary(sf)\nlibrary(tmap)\nlibrary(grid)\nlibrary(gridtext)\n\n# Read in data\nauthor_stats = read_xlsx(here(\"data\", \"authors.xlsx\"), sheet = 1) %&gt;%\n  clean_names() %&gt;% \n  mutate(gender = str_to_title(gender))\n\n# Read in countries geometries\ncountry_bounds &lt;- read_sf(here(\"data\", \"countries\", \"world-administrative-boundaries.shp\")) %&gt;% \n  select(iso3, name, geometry)\n\n# Enable showtext\nshowtext_auto()\n\n# Ensure showtext is used\nshowtext_opts(dpi = 300)\n\n# Import font\nfont_add_google(name = \"Lexend\", family = \"lexend\")\n\n\n\n\n\n\nData wrangling\n# Calculate summary stats grouped by field\ncountries &lt;- author_stats %&gt;% \n  group_by(country, gender) %&gt;% \n  summarise(country_gender = sum(authors)) %&gt;% \n  mutate(total_authors = sum(country_gender),\n         percent_country = country_gender/total_authors)\n\n\n# pivot wider to add columns of the number of authors by field by men or women\ncountries_wide &lt;- countries %&gt;%\n  pivot_wider(\n    id_cols = c(country, total_authors),\n    names_from = gender,\n    values_from = c(country_gender, percent_country),\n    names_prefix = \"\"\n  ) %&gt;%\n  group_by(country) %&gt;%\n  mutate(total_authors = first(total_authors),\n         gender_gap = country_gender_Men - country_gender_Women,\n         percent_gender_gap = percent_country_Men - percent_country_Women,\n         gender_ratio = country_gender_Men/country_gender_Women) %&gt;%\n  ungroup() \n\n# Join countries geometries\ncountries_wide &lt;- left_join(countries_wide, country_bounds, by = join_by(country== iso3))\n\n# Reassert that it's a sf data frame\ncountries_wide &lt;- countries_wide %&gt;% \n  st_as_sf() %&gt;% \n  drop_na() %&gt;% \n  filter(!name %in% c(\"Azores Islands\")) # remove extra country \n\n# Decided to bin the gender ratio to make the visualization easier\ncountries_new &lt;- countries_wide %&gt;% \n  mutate(gender_ratio = round(gender_ratio / 0.5) * 0.5)\n\n\n\n\nMapping code\n# Map of average publications by country\nmap &lt;- tm_shape(countries_new) +\n  tm_fill(\"gender_ratio\",\n          palette = c(\"#F2C5FD\", \"#EC9BFC\", \"#D67BE6\", \n                       \"#A93CBA\", \"#8C2AA7\", \"#6A1E99\"),\n          breaks = c(0, 1, 1.5, 2, 2.5, 7),\n          labels = c('1 : 1', '1 : 1.5', '1 : 2', '1 : 2.5', '1 : 7'),\n          title = \"Ratio of Female : Male\\nAuthors\") +\n  tm_shape(country_bounds) +\n  tm_borders(col = \"black\", lwd = 0.8) +\n  tm_layout(main.title = \"Difference in Average Publications Across Countries\\nBetween Men and Women\",\n            main.title.size = 1.8, \n            main.title.position = \"center\",\n            main.title.fontface = \"bold\",\n            bg.color = \"transparent\",\n            inner.margins = c(0, 0, 0, 0),  # Adjust margins (top, right, bottom, left)\n            legend.position = c(0, 0),\n            legend.text.size = 1.1,\n            legend.title.size = 1.1,\n            fontfamily = \"lexend\",\n            frame = FALSE)\n\nmap\n\n\n\n\n\n\n\n\nFigure 1: The gender publishing gap by country\n\n\n\n\n\n\n\n\nData wrangling\n# Filter data into the two different time periods\n\n# Older data\nauthor_old &lt;- author_stats %&gt;% \n  filter(period == \"1999-2003\")\n\n# Generate some summary stats\nold_sum &lt;- author_old %&gt;% \n  group_by(gender) %&gt;% \n  summarise(gender_authors = sum(authors)) %&gt;% \n  ungroup() %&gt;% \n  mutate(total_authors = sum(gender_authors), \n         percent_pub = gender_authors/total_authors)\n\n# More recent data\nauthor_new &lt;- author_stats %&gt;% \n  filter(period == \"2014-2018\")\n\n# Generate some summary stats\nnew_sum &lt;- author_new %&gt;% \n  group_by(gender) %&gt;% \n  summarise(gender_authors = sum(authors)) %&gt;% \n  ungroup() %&gt;% \n  mutate(total_authors = sum(gender_authors), \n         percent_pub = gender_authors/total_authors)\n\n\n\n\nPie chart code\n# Pie chart for the older years\nold_pie &lt;- ggplot(old_sum, aes(x = \"\", y = percent_pub, fill = gender)) +\n  geom_bar(stat = \"identity\", width = 1) +\n  coord_polar(\"y\", start = 0) +\n  geom_text(aes(label = paste0(gender, \"\\n\", scales::percent(percent_pub, accuracy = 1)), family = \"lexend\"),\n            position = position_stack(vjust = 0.5),\n            color = \"white\", size = 6) +\n  scale_fill_manual(values = c(\"Women\" = \"#ec9bfc\", \"Men\" = \"#6A1E99\")) + \n  theme_void() +  \n  labs(subtitle = \"1993-2003\",\n       fill = \"\") +\n  theme(\n    text = element_text(family = \"lexend\"),\n    legend.text = element_text(family = \"lexend\"),\n    plot.subtitle = element_text(family = \"lexend\", hjust = 0.5),\n    legend.position = \"none\"\n  )\n\n# Pie chart for the more recent years\nnew_pie &lt;- ggplot(new_sum, aes(x = \"\", y = percent_pub, fill = gender)) +\n  geom_bar(stat = \"identity\", width = 1) + \n  coord_polar(\"y\", start = 0) +\n  geom_text(aes(label = paste0(gender, \"\\n\", scales::percent(percent_pub, accuracy = 1)), family = \"lexend\"),\n            position = position_stack(vjust = 0.5),\n            color = \"white\", size = 6) +\n  scale_fill_manual(values = c(\"Women\" = \"#ec9bfc\", \"Men\" = \"#6A1E99\")) + \n  theme_void() +  \n  labs(subtitle = \"2014-2018\",\n       fill = \"\") +\n  theme(\n    text = element_text(family = \"lexend\"),\n    legend.text = element_text(family = \"lexend\"),\n    plot.subtitle = element_text(family = \"lexend\", hjust = 0.5),\n    legend.position = \"none\"\n  )\n\n# Stick the two figures together\npatchwork &lt;- old_pie + new_pie\npatch_final &lt;- patchwork + plot_annotation(\n  title = \"Women have increased their share of academic publications\\nby 10% from 1993 to 2018\",\n  caption = \"Source: Elsevier Data Repository | Graphic: Haylee Oyler\"\n) &\n  theme( \n    text = element_text(family = \"lexend\"),\n    plot.title = element_text(face = \"bold\", size = 20),\n    plot.subtitle = element_text(size = 16),\n    plot.background = element_rect(fill = \"white\", color = NA),\n    legend.text = element_text(size=16, face = \"bold\"),\n    plot.caption = element_text(size=10),\n    plot.background = element_rect(fill = \"transparent\", color = NA),\n    panel.background = element_rect(fill = \"transparent\", color = NA),\n    legend.background = element_rect(fill = \"transparent\", color = NA), \n    legend.key = element_rect(fill = \"transparent\", color = NA) \n    )\npatch_final\n\n\n\n\n\n\n\n\nFigure 2: The gender publishing gap over time\n\n\n\n\n\n\n\n\nData wrangling\n# Calculate summary stats grouped by field\nfields &lt;- author_stats %&gt;% \n  group_by(subject_area_or_subfield, gender) %&gt;% \n  summarise(field_gender = sum(authors)) %&gt;% \n  mutate(total_authors = sum(field_gender),\n         percent_field = field_gender/total_authors)\n\n\n# pivot wider to add columns of the number of authors by field by men or women\nfields_wide &lt;- fields %&gt;%\n  pivot_wider(\n    id_cols = c(subject_area_or_subfield, total_authors),\n    names_from = gender,\n    values_from = c(field_gender, percent_field),\n    names_prefix = \"\"\n  ) %&gt;%\n  group_by(subject_area_or_subfield) %&gt;%\n  mutate(total_authors = first(total_authors),\n         gender_gap = field_gender_Men - field_gender_Women,\n         percent_gender_gap = percent_field_Men - percent_field_Women) %&gt;%\n  ungroup() %&gt;% \n  filter(subject_area_or_subfield != \"ALL\")\n\n# Reorder data by the gender gap from high to low\nfields_wide &lt;- fields_wide %&gt;% \n  mutate(subject_area_or_subfield = fct_reorder(.f = subject_area_or_subfield, \n                                                .x = percent_gender_gap))\n\n\n\n\nDumbbell plot code\n# Create color-coded plot title\ndumbbell_title &lt;- glue::glue(\"&lt;span style='color:#6A1E99;'&gt;**Men**&lt;/span&gt; \n                       Publish More than \n                       &lt;span style='color:#ec9bfc;'&gt;**Women**&lt;/span&gt;\n                       Across\\nMost Academic Fields\")\n\n\n# dumbbell plot\ndumbbell_plot &lt;- ggplot(fields_wide) +\n  geom_linerange(aes(y = subject_area_or_subfield,\n                     xmin = percent_field_Women, \n                     xmax = percent_field_Men)) +\n  geom_point(aes(x = percent_field_Women, \n                 y = subject_area_or_subfield, \n                 color = \"Women\"), size = 3.5) +\n  geom_point(aes(x = percent_field_Men, \n                 y = subject_area_or_subfield, \n                 color = \"Men\"),\n             size = 3.5) +\n  geom_vline(xintercept = .5, linetype = \"dashed\", color = \"gray40\") +\n  scale_x_continuous(breaks = seq(0, 1, by = 0.1),\n                     labels = scales::percent_format(scale = 100)) +  \n  scale_color_manual(values = c(\"Women\" = \"#ec9bfc\", \"Men\" = \"#6A1E99\")) + \n  labs(title = dumbbell_title, \n      x = \"Percentage of Total Publications\",\n      y = \"\") +\n  theme_minimal(base_size = 20)  + \n    theme(\n    text = element_text(family = \"lexend\"),\n    plot.title = ggtext::element_textbox(face = \"bold\", \n                                         margin = margin(t = 0,r = 0, \n                                                         b = 0, l = -130),\n                                         padding = margin(t = 0, r = 0, \n                                                          b = 15, l = 0)),\n    axis.title =  ggtext::element_textbox(size = rel(1.1),\n                                          color = \"black\",\n                                          padding = margin(t = 10, r = 0, \n                                                           b = 0, l = 0)),\n    axis.text.x = element_text(size=rel(1), face = \"bold\"),\n    axis.text.y = element_text(size=rel(0.9), color = \"black\"),\n    plot.caption = element_text(size=rel(0.6)),\n    panel.grid.minor.x = element_blank(),\n    legend.position=\"none\"\n    )\n\ndumbbell_plot\n\n\n\n\n\n\n\n\nFigure 3: The gender publishing gap across disciplines"
  },
  {
    "objectID": "posts/2024-12-01-thomas-fire/thomas-fire-analysis.html",
    "href": "posts/2024-12-01-thomas-fire/thomas-fire-analysis.html",
    "title": "Thomas Fire AQI and Burn Scar",
    "section": "",
    "text": "Image credits: LA Daily News\nAuthor: Haylee Oyler\nThis project contains two analyses of the 2017 Thomas Fire in Santa Barbara and Ventura Counties.\n\nPart 1: Visualizing AQI during the 2017 Thomas Fire in Santa Barbara County uses air quality index data to examine the change in air quality before and after the fire.\nPart 2: False Color Imagery of the 2017 Thomas Fire uses geospatial Landsat and fire perimeter data to create a false color map of the residual fire scar.\n\nAdditional information can be found at the GitHub repository for this project."
  },
  {
    "objectID": "posts/2024-12-01-thomas-fire/thomas-fire-analysis.html#import-aqi-data-and-explore",
    "href": "posts/2024-12-01-thomas-fire/thomas-fire-analysis.html#import-aqi-data-and-explore",
    "title": "Thomas Fire AQI and Burn Scar",
    "section": "Import AQI data and explore",
    "text": "Import AQI data and explore\nWe’ll start by importing our air quality index data for 2017 and 2018 and conducting a preliminary exploration.\n\n# Read in AQI data for both years\naqi_17 = pd.read_csv('https://aqs.epa.gov/aqsweb/airdata/daily_aqi_by_county_2017.zip', \n                    compression = 'zip')\n\naqi_18 = pd.read_csv('https://aqs.epa.gov/aqsweb/airdata/daily_aqi_by_county_2018.zip', \n                    compression = 'zip')\n\n\n# View the first few rows of aqi 2017\naqi_17.head(3)\n\n\n\n\n\n\n\n\nState Name\ncounty Name\nState Code\nCounty Code\nDate\nAQI\nCategory\nDefining Parameter\nDefining Site\nNumber of Sites Reporting\n\n\n\n\n0\nAlabama\nBaldwin\n1\n3\n2017-01-01\n28\nGood\nPM2.5\n01-003-0010\n1\n\n\n1\nAlabama\nBaldwin\n1\n3\n2017-01-04\n29\nGood\nPM2.5\n01-003-0010\n1\n\n\n2\nAlabama\nBaldwin\n1\n3\n2017-01-10\n25\nGood\nPM2.5\n01-003-0010\n1\n\n\n\n\n\n\n\n\n# View the first few rows of aqi 2018\naqi_18.head(3)\n\n\n\n\n\n\n\n\nState Name\ncounty Name\nState Code\nCounty Code\nDate\nAQI\nCategory\nDefining Parameter\nDefining Site\nNumber of Sites Reporting\n\n\n\n\n0\nAlabama\nBaldwin\n1\n3\n2018-01-02\n42\nGood\nPM2.5\n01-003-0010\n1\n\n\n1\nAlabama\nBaldwin\n1\n3\n2018-01-05\n45\nGood\nPM2.5\n01-003-0010\n1\n\n\n2\nAlabama\nBaldwin\n1\n3\n2018-01-08\n20\nGood\nPM2.5\n01-003-0010\n1\n\n\n\n\n\n\n\n\n# View unique defining parameters of the aqi data \naqi_17['Defining Parameter'].unique()\n\narray(['PM2.5', 'Ozone', 'NO2', 'PM10', 'CO'], dtype=object)\n\n\n\n# View the info of the aqi data\naqi_17.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 326801 entries, 0 to 326800\nData columns (total 10 columns):\n #   Column                     Non-Null Count   Dtype \n---  ------                     --------------   ----- \n 0   State Name                 326801 non-null  object\n 1   county Name                326801 non-null  object\n 2   State Code                 326801 non-null  int64 \n 3   County Code                326801 non-null  int64 \n 4   Date                       326801 non-null  object\n 5   AQI                        326801 non-null  int64 \n 6   Category                   326801 non-null  object\n 7   Defining Parameter         326801 non-null  object\n 8   Defining Site              326801 non-null  object\n 9   Number of Sites Reporting  326801 non-null  int64 \ndtypes: int64(4), object(6)\nmemory usage: 24.9+ MB\n\n\nOur AQI data contains information about the state and county location, date, and air quality index. We can also see that the defining parameter of air pollution is either PM 2.5, ozone, NO2, PM10, or CO. Additionally, our AQI data has no missing values and 4 of our variables are type int64 and 6 are type object. Something notable about these data types is that our Date column is not being recognized as a datetime object. We will fix this later on…"
  },
  {
    "objectID": "posts/2024-12-01-thomas-fire/thomas-fire-analysis.html#clean-the-aqi-data",
    "href": "posts/2024-12-01-thomas-fire/thomas-fire-analysis.html#clean-the-aqi-data",
    "title": "Thomas Fire AQI and Burn Scar",
    "section": "Clean the AQI data",
    "text": "Clean the AQI data\nCurrently, our AQI data is housed in two separate data frames. We will join them together using the pandas function pd.concat() and save them as one data frame named aqi.\nNOTE: When we concatenate data frames without any extra parameters specified in pd.concat(), the indices are simply stacked on top of one another. Therefore, the resulting index values of aqi will not match the length of the new data frame.\n\n# Bind 2017 and 2018 AQI data together\naqi = pd.concat([aqi_17, aqi_18])\naqi.head(3)\n\n\n\n\n\n\n\n\nState Name\ncounty Name\nState Code\nCounty Code\nDate\nAQI\nCategory\nDefining Parameter\nDefining Site\nNumber of Sites Reporting\n\n\n\n\n0\nAlabama\nBaldwin\n1\n3\n2017-01-01\n28\nGood\nPM2.5\n01-003-0010\n1\n\n\n1\nAlabama\nBaldwin\n1\n3\n2017-01-04\n29\nGood\nPM2.5\n01-003-0010\n1\n\n\n2\nAlabama\nBaldwin\n1\n3\n2017-01-10\n25\nGood\nPM2.5\n01-003-0010\n1\n\n\n\n\n\n\n\nNotice that our data frame dimensions of 654344 rows by 10 columns do not match the last row index of 327542. To address our confusing index, we will change the index of our data frame to the date column.\nFirst, we will ensure that our Date column is a pandas datetime object. Then, we will set our index to the Date column.\n\n# Convert date to a datetime object\naqi.Date = pd.to_datetime(aqi.Date)\n\n# Set the index to our datetime to make visualizing easier later on\naqi = aqi.set_index('Date')\naqi.head(3)\n\n\n\n\n\n\n\n\nState Name\ncounty Name\nState Code\nCounty Code\nAQI\nCategory\nDefining Parameter\nDefining Site\nNumber of Sites Reporting\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n\n\n\n2017-01-01\nAlabama\nBaldwin\n1\n3\n28\nGood\nPM2.5\n01-003-0010\n1\n\n\n2017-01-04\nAlabama\nBaldwin\n1\n3\n29\nGood\nPM2.5\n01-003-0010\n1\n\n\n2017-01-10\nAlabama\nBaldwin\n1\n3\n25\nGood\nPM2.5\n01-003-0010\n1\n\n\n\n\n\n\n\nNext, we will clean the column names of our new data frame. We will make all the column names lower snake case via the operations below. Here is a step-by-step of what the functions do:\n\naqi.columns = (aqi.columns selects the columns from the aqi data frame and reassigns them to the original data frame\n.str.lower() uses the string operator to make all the letters lowercase\n.str.replace(' ','_') converts the output of the lowercase columns to a string and replaces all spaces with an underscore\n) closes the method chaining\nprint(aqi.columns, '\\n') lets us view the output of our modified column names\n\n\n# Initial column names: notice caps and spaces\nprint(aqi.columns, '\\n')\n\n# Simplify column names\naqi.columns = (aqi.columns\n                  .str.lower()\n                  .str.replace(' ','_')\n                )\nprint(aqi.columns, '\\n')\n\nIndex(['State Name', 'county Name', 'State Code', 'County Code', 'AQI',\n       'Category', 'Defining Parameter', 'Defining Site',\n       'Number of Sites Reporting'],\n      dtype='object') \n\nIndex(['state_name', 'county_name', 'state_code', 'county_code', 'aqi',\n       'category', 'defining_parameter', 'defining_site',\n       'number_of_sites_reporting'],\n      dtype='object')"
  },
  {
    "objectID": "posts/2024-12-01-thomas-fire/thomas-fire-analysis.html#filter-aqi-data",
    "href": "posts/2024-12-01-thomas-fire/thomas-fire-analysis.html#filter-aqi-data",
    "title": "Thomas Fire AQI and Burn Scar",
    "section": "Filter AQI data",
    "text": "Filter AQI data\nFor this specific analysis, we’re only interested in the air quality Santa Barbara County. We will filter our data frame to Santa Barbara and drop columns with unnecessary information.\n\n# Filter data to Santa Barbara county \naqi_sb = aqi[aqi['county_name'] == 'Santa Barbara']\n\n# Drop the columns we're not interested in working with\naqi_sb = aqi_sb.drop(['state_name', 'county_name', 'state_code', 'county_code'], axis=1)\naqi_sb.head(3)\n\n\n\n\n\n\n\n\naqi\ncategory\ndefining_parameter\ndefining_site\nnumber_of_sites_reporting\n\n\nDate\n\n\n\n\n\n\n\n\n\n2017-01-01\n39\nGood\nOzone\n06-083-4003\n12\n\n\n2017-01-02\n39\nGood\nPM2.5\n06-083-2011\n11\n\n\n2017-01-03\n71\nModerate\nPM10\n06-083-4003\n12"
  },
  {
    "objectID": "posts/2024-12-01-thomas-fire/thomas-fire-analysis.html#aqi-rolling-average",
    "href": "posts/2024-12-01-thomas-fire/thomas-fire-analysis.html#aqi-rolling-average",
    "title": "Thomas Fire AQI and Burn Scar",
    "section": "AQI rolling average",
    "text": "AQI rolling average\nIn the next cell we will calculate an average over a rolling window using the rolling() method for pandas.Series:\n\nrolling() is a lazy method, so we need to specify what we want to calculate over each window before it does something.\nin this example, we use the aggregator function mean() to calculate the average over each window\nthe parameter ‘5D’ indicates we want the window for our rolling average to be 5 days.\nwe get a pandas.Series as the output\n\n\n# Calculate AQI rolling average over 5 days\nrolling_average = aqi_sb['aqi'].rolling(window='5D').mean()\n\n\n# Append our rolling average to our original data frame\naqi_sb['five_day_average'] = rolling_average"
  },
  {
    "objectID": "posts/2024-12-01-thomas-fire/thomas-fire-analysis.html#plot-aqi-during-the-thomas-fire",
    "href": "posts/2024-12-01-thomas-fire/thomas-fire-analysis.html#plot-aqi-during-the-thomas-fire",
    "title": "Thomas Fire AQI and Burn Scar",
    "section": "Plot AQI during the Thomas Fire",
    "text": "Plot AQI during the Thomas Fire\nNow that our data frame contains all the correct, necessary information, we can visualize it using matplotlib\n\n\nPlot the AQI\n# Initialize an empty figure (fig) and axis (ax)\nfig, ax = plt.subplots()\n\n# Visualize air quality during the Thomas Fire\naqi_sb.aqi.plot(ax=ax, label = 'AQI') # daily aqi\naqi_sb.five_day_average.plot(ax=ax, label = \"Five day AQI average\") # five day average aqi\n\n# Show the date of the Thomas fire\nplt.axvline(x = '2017-12-04', \n            color = 'red', \n            linestyle = 'dashed', \n            label = \"Thomas Fire\")\n\n# Customize the plot\nax.set_title('Daily AQI and 5-day AQI averages during the\\nThomas Fire in Santa Barbara County')\nax.set_xlabel('Date')\nax.set_ylabel('AQI')\nax.legend()\n\n# Display the figure\nplt.show()\n\n\n\n\n\n\n\n\n\nThis plot shows the drastic spike in AQI shortly after the Thomas fire in December of 2017. The daily AQI peaks at just over 300, which is into the hazardous classification, and the five day average AQI is around 220, which is in the unhealthy classification. Wildfire’s are known to release many harmful chemicals and particulates that contribute to worsening air quality. This in turn leads to worsening health outcomes for illnesses such as respiratory disease, heart disease, asthma, and more."
  },
  {
    "objectID": "posts/2024-12-01-thomas-fire/thomas-fire-analysis.html#about-1",
    "href": "posts/2024-12-01-thomas-fire/thomas-fire-analysis.html#about-1",
    "title": "Thomas Fire AQI and Burn Scar",
    "section": "About",
    "text": "About\n\nPurpose\nPart 2 of this analysis details the steps to visualize Landsat multispectral geospatial data for the 2017 Thomas Fire. False color imagery, created using satellite data from instruments like Landsat, is a useful tool for monitoring wildfire impacts. By assigning infrared bands to visible colors, these images highlight vegetation health, burn severity, and the extent of fire scars. This approach helps researchers and land managers assess recovery efforts, identify high-risk areas, and plan restoration strategies.\nPart 2 will create a false color image of the Thomas Fire using remote sensing data, highlighting the fire scar and exploring how coding and data visualization support environmental monitoring.\n\n\nHighlights\n\nImport Thomas fire perimeter data with geopandas and os\nImport Landsat data with rioxarray and os\nExplore and clean geospatial data with pandas and rioxarray\nConstruct a true color image of the Thomas Fire with rioxarray\nConstruct a false color image of the Thomas Fire with rioxarray\nVisualize the Thomas Fire false color scar with the fire perimeter data using matplotlib\n\n\n\nAbout the Data\nThe Landsat data is a simplified collection of bands (red, green, blue, near-infrared and shortwave infrared) from the Landsat Collection 2 Level-2 atmospherically corrected surface reflectance data, collected by the Landsat 8 satellite. It was pre-processed in the Microsoft Planetary data catalogue to remove data outside land and coarsen the spatial resolution\nThe Thomas Fire perimeter data comes from CalFire’s data portal. CalFire is the department of forestry and fire protection. They have a Geodatabase of all historical fire perimeters in the state of California from 1878 until present. The database includes information on the fire date, managing agency, cause, acres, and the geospatial boundary of the fire, among other information. This data was pre-processed to select only the Thomas fire boundary geometry."
  },
  {
    "objectID": "posts/2024-12-01-thomas-fire/thomas-fire-analysis.html#import-geospatial-data-and-explore",
    "href": "posts/2024-12-01-thomas-fire/thomas-fire-analysis.html#import-geospatial-data-and-explore",
    "title": "Thomas Fire AQI and Burn Scar",
    "section": "Import geospatial data and explore",
    "text": "Import geospatial data and explore\n\n# Import Landsat nc data\nlandsat = rioxr.open_rasterio(os.path.join('data',\n                                    'landsat8-2018-01-26-sb-simplified.nc')\n                                    )\nlandsat = landsat.rio.write_crs(\"EPSG:4326\")\n\n# Import fire perimeter data\nthomas_boundary = gpd.read_file(os.path.join('data',\n                                    'thomas_boundary.geojson')\n                                    )\n\n\n# View the Landsat data\nlandsat\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 25MB\nDimensions:      (band: 1, x: 870, y: 731)\nCoordinates:\n  * band         (band) int64 8B 1\n  * x            (x) float64 7kB 0.5 1.5 2.5 3.5 4.5 ... 866.5 867.5 868.5 869.5\n  * y            (y) float64 6kB 0.5 1.5 2.5 3.5 4.5 ... 727.5 728.5 729.5 730.5\n    spatial_ref  int64 8B 0\nData variables:\n    blue         (band, y, x) float64 5MB ...\n    green        (band, y, x) float64 5MB ...\n    nir08        (band, y, x) float64 5MB ...\n    red          (band, y, x) float64 5MB ...\n    swir22       (band, y, x) float64 5MB ...\nAttributes: (12/78)\n    blue_add_offset:                               0\n    blue_coordinates:                              time\n    blue_grid_mapping:                             spatial_ref\n    blue_scale_factor:                             1\n    blue__FillValue:                               0\n    blue__Netcdf4Coordinates:                      0 1\n    ...                                            ...\n    y_standard_name:                               projection_y_coordinate\n    y_units:                                       metre\n    y__FillValue:                                  nan\n    y__Netcdf4Coordinates:                         0\n    y__Netcdf4Dimid:                               0\n    _NCProperties:                                 version=2,netcdf=4.9.2,hdf...xarray.DatasetDimensions:band: 1x: 870y: 731Coordinates: (4)band(band)int641array([1])x(x)float640.5 1.5 2.5 ... 867.5 868.5 869.5array([5.000e-01, 1.500e+00, 2.500e+00, ..., 8.675e+02, 8.685e+02, 8.695e+02],\n      shape=(870,))y(y)float640.5 1.5 2.5 ... 728.5 729.5 730.5array([5.000e-01, 1.500e+00, 2.500e+00, ..., 7.285e+02, 7.295e+02, 7.305e+02],\n      shape=(731,))spatial_ref()int640crs_wkt :GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :WGS 84horizontal_datum_name :World Geodetic System 1984grid_mapping_name :latitude_longitudespatial_ref :GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]GeoTransform :0.0 1.0 0.0 0.0 0.0 1.0array(0)Data variables: (5)blue(band, y, x)float64...add_offset :0.0coordinates :timescale_factor :1.0_Netcdf4Coordinates :0 1_Netcdf4Dimid :0_FillValue :0.0[635970 values with dtype=float64]green(band, y, x)float64...add_offset :0.0coordinates :timescale_factor :1.0_Netcdf4Coordinates :0 1_Netcdf4Dimid :0_FillValue :0.0[635970 values with dtype=float64]nir08(band, y, x)float64...add_offset :0.0coordinates :timescale_factor :1.0_Netcdf4Coordinates :0 1_Netcdf4Dimid :0_FillValue :0.0[635970 values with dtype=float64]red(band, y, x)float64...add_offset :0.0coordinates :timescale_factor :1.0_Netcdf4Coordinates :0 1_Netcdf4Dimid :0_FillValue :0.0[635970 values with dtype=float64]swir22(band, y, x)float64...add_offset :0.0coordinates :timescale_factor :1.0_Netcdf4Coordinates :0 1_Netcdf4Dimid :0_FillValue :0.0[635970 values with dtype=float64]Indexes: (3)bandPandasIndexPandasIndex(Index([1], dtype='int64', name='band'))xPandasIndexPandasIndex(Index([  0.5,   1.5,   2.5,   3.5,   4.5,   5.5,   6.5,   7.5,   8.5,   9.5,\n       ...\n       860.5, 861.5, 862.5, 863.5, 864.5, 865.5, 866.5, 867.5, 868.5, 869.5],\n      dtype='float64', name='x', length=870))yPandasIndexPandasIndex(Index([  0.5,   1.5,   2.5,   3.5,   4.5,   5.5,   6.5,   7.5,   8.5,   9.5,\n       ...\n       721.5, 722.5, 723.5, 724.5, 725.5, 726.5, 727.5, 728.5, 729.5, 730.5],\n      dtype='float64', name='y', length=731))Attributes: (78)blue_add_offset :0blue_coordinates :timeblue_grid_mapping :spatial_refblue_scale_factor :1blue__FillValue :0blue__Netcdf4Coordinates :0 1blue__Netcdf4Dimid :0green_add_offset :0green_coordinates :timegreen_grid_mapping :spatial_refgreen_scale_factor :1green__FillValue :0green__Netcdf4Coordinates :0 1green__Netcdf4Dimid :0nir08_add_offset :0nir08_coordinates :timenir08_grid_mapping :spatial_refnir08_scale_factor :1nir08__FillValue :0nir08__Netcdf4Coordinates :0 1nir08__Netcdf4Dimid :0red_add_offset :0red_coordinates :timered_grid_mapping :spatial_refred_scale_factor :1red__FillValue :0red__Netcdf4Coordinates :0 1red__Netcdf4Dimid :0spatial_ref_crs_wkt :PROJCS[\"WGS 84 / UTM zone 11N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-117],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"32611\"]]spatial_ref_false_easting :500000spatial_ref_false_northing :0spatial_ref_geographic_crs_name :WGS 84spatial_ref_GeoTransform :121170.0 90.0 0.0 3952530.0 0.0 -90.0spatial_ref_grid_mapping_name :transverse_mercatorspatial_ref_horizontal_datum_name :World Geodetic System 1984spatial_ref_inverse_flattening :298.257223563spatial_ref_latitude_of_projection_origin :0spatial_ref_longitude_of_central_meridian :-117spatial_ref_longitude_of_prime_meridian :0spatial_ref_prime_meridian_name :Greenwichspatial_ref_projected_crs_name :WGS 84 / UTM zone 11Nspatial_ref_reference_ellipsoid_name :WGS 84spatial_ref_scale_factor_at_central_meridian :0.9996spatial_ref_semi_major_axis :6378137spatial_ref_semi_minor_axis :6356752.31424518spatial_ref_spatial_ref :PROJCS[\"WGS 84 / UTM zone 11N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-117],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"32611\"]]swir22_add_offset :0swir22_coordinates :timeswir22_grid_mapping :spatial_refswir22_scale_factor :1swir22__FillValue :0swir22__Netcdf4Coordinates :0 1swir22__Netcdf4Dimid :0x_axis :Xx_CLASS :DIMENSION_SCALEx_crs :EPSG:32611x_long_name :x coordinate of projectionx_NAME :xx_REFERENCE_LIST :x_resolution :30x_standard_name :projection_x_coordinatex_units :metrex__FillValue :nanx__Netcdf4Coordinates :1x__Netcdf4Dimid :1y_axis :Yy_CLASS :DIMENSION_SCALEy_crs :EPSG:32611y_long_name :y coordinate of projectiony_NAME :yy_REFERENCE_LIST :y_resolution :-30y_standard_name :projection_y_coordinatey_units :metrey__FillValue :nany__Netcdf4Coordinates :0y__Netcdf4Dimid :0_NCProperties :version=2,netcdf=4.9.2,hdf5=1.14.0\n\n\n\n\nLandsat info\n# Examine raster attributes using rio accessor\nprint('Height: ', landsat.rio.height)\nprint('Width: ', landsat.rio.width, '\\n')\n\nprint('Spatial bounding box: ')\nprint(landsat.rio.bounds(), '\\n')\n\nprint('CRS: ', landsat.rio.crs)\n\n\nHeight:  731\nWidth:  870 \n\nSpatial bounding box: \n(0.0, 0.0, 870.0, 731.0) \n\nCRS:  EPSG:4326\n\n\n\nLandsat data description\nOur Landsat data contains the variables red, green, blue, nir08, and swir22. These are different bands of our lansat data. The dimensions of our data for each band are an (x,y) coordinate of projection of (870, 731). The CRS is EPSG: 32611 and the height and width of the data are 731 and 870. Each variable in our dataset contains the dimensions (band, y, x).\n\nthomas_boundary.head()\n\n\n\n\n\n\n\n\nyear\nstate\nagency\nunit_id\nfire_name\ninc_num\nirwinid\nalarm_date\ncont_date\nc_method\ncause\nobjective\ncomplex_name\ncomplex_id\ncomments\nfire_num\nshape_length\nshape_area\ngeometry\n\n\n\n\n0\n2017.0\nCA\nUSF\nVNC\nTHOMAS\n00003583\n\n2017-12-04 00:00:00+00:00\n2018-01-12 00:00:00+00:00\n7.0\n9.0\n1.0\nNone\nNone\nCONT_DATE based on Inciweb\nNone\n445282.444798\n1.140367e+09\nMULTIPOLYGON (((34867.386 -396856.457, 34819.3...\n\n\n\n\n\n\n\n\nthomas_boundary.info()\n\n&lt;class 'geopandas.geodataframe.GeoDataFrame'&gt;\nRangeIndex: 1 entries, 0 to 0\nData columns (total 19 columns):\n #   Column        Non-Null Count  Dtype              \n---  ------        --------------  -----              \n 0   year          1 non-null      float64            \n 1   state         1 non-null      object             \n 2   agency        1 non-null      object             \n 3   unit_id       1 non-null      object             \n 4   fire_name     1 non-null      object             \n 5   inc_num       1 non-null      object             \n 6   irwinid       1 non-null      object             \n 7   alarm_date    1 non-null      datetime64[ms, UTC]\n 8   cont_date     1 non-null      datetime64[ms, UTC]\n 9   c_method      1 non-null      float64            \n 10  cause         1 non-null      float64            \n 11  objective     1 non-null      float64            \n 12  complex_name  0 non-null      object             \n 13  complex_id    0 non-null      object             \n 14  comments      1 non-null      object             \n 15  fire_num      0 non-null      object             \n 16  shape_length  1 non-null      float64            \n 17  shape_area    1 non-null      float64            \n 18  geometry      1 non-null      geometry           \ndtypes: datetime64[ms, UTC](2), float64(6), geometry(1), object(10)\nmemory usage: 284.0+ bytes\n\n\n\nthomas_boundary.crs\n\n&lt;Projected CRS: EPSG:3310&gt;\nName: NAD83 / California Albers\nAxis Info [cartesian]:\n- X[east]: Easting (metre)\n- Y[north]: Northing (metre)\nArea of Use:\n- name: United States (USA) - California.\n- bounds: (-124.45, 32.53, -114.12, 42.01)\nCoordinate Operation:\n- name: California Albers\n- method: Albers Equal Area\nDatum: North American Datum 1983\n- Ellipsoid: GRS 1980\n- Prime Meridian: Greenwich\n\n\n\n\nFire perimeter data description\nThis fire perimeter data comes from CalFire and includes data for all fire perimeters from 1878 to 2023. It has data on the year, the fire name, the reporting agency, the cause, and the duration, among other data. The CRS is NAD83 California Albers and it is a projected CRS (EPSG:3310)"
  },
  {
    "objectID": "posts/2024-12-01-thomas-fire/thomas-fire-analysis.html#clean-the-landsat-data",
    "href": "posts/2024-12-01-thomas-fire/thomas-fire-analysis.html#clean-the-landsat-data",
    "title": "Thomas Fire AQI and Burn Scar",
    "section": "Clean the Landsat data",
    "text": "Clean the Landsat data\n\n# Remove the band dimension and variable\nlandsat = landsat.squeeze().drop_vars('band')\n\n# Confirm it was removed correctly\nprint(landsat.sizes)\n\nFrozen({'x': 870, 'y': 731})"
  },
  {
    "objectID": "posts/2024-12-01-thomas-fire/thomas-fire-analysis.html#visualize-the-thomas-fire-with-true-color-imagery",
    "href": "posts/2024-12-01-thomas-fire/thomas-fire-analysis.html#visualize-the-thomas-fire-with-true-color-imagery",
    "title": "Thomas Fire AQI and Burn Scar",
    "section": "Visualize the Thomas Fire with true color imagery",
    "text": "Visualize the Thomas Fire with true color imagery\n\n# First attempt to visualize the Landsat data \nlandsat[['red', 'green', 'blue']].to_array().plot.imshow()\n\n\nOur first attempt to map the data didn’t go quite as planned. Let’s try adjusting the robust parameter of .imshow() and see what happens\n\n# Visualize the Landsat data using true color imagery\nlandsat[['red', 'green', 'blue']].to_array().plot.imshow(robust=True)\n\n\nAfter we adjusted the scale for plotting the bands, we got a much more comprehensible image. The clouds were throwing off the scale for plotting. The robust=True argument allows us to infer a different set vmin and vmax values to properly color the image. It takes out the 2nd and 98th percentile, removing outliers which makes it easier to visualize.\nNext, we will use false color imagery to view the fire…"
  },
  {
    "objectID": "posts/2024-12-01-thomas-fire/thomas-fire-analysis.html#visualize-the-thomas-fire-with-false-color-imagery",
    "href": "posts/2024-12-01-thomas-fire/thomas-fire-analysis.html#visualize-the-thomas-fire-with-false-color-imagery",
    "title": "Thomas Fire AQI and Burn Scar",
    "section": "Visualize the Thomas Fire with false color imagery",
    "text": "Visualize the Thomas Fire with false color imagery\nTo make the Thomas Fire burn scar more visible, we will use false color imagery by assigning short-wave infrared radiation to red, near infrared to green, and red to blue.\n\n# Visualize the Landsat data using false color imagery\nlandsat[['swir22', 'nir08', 'red']].to_array().plot.imshow(robust=True)"
  },
  {
    "objectID": "posts/2024-12-01-thomas-fire/thomas-fire-analysis.html#map-the-thomas-fire-scar-and-boundary",
    "href": "posts/2024-12-01-thomas-fire/thomas-fire-analysis.html#map-the-thomas-fire-scar-and-boundary",
    "title": "Thomas Fire AQI and Burn Scar",
    "section": "Map the Thomas Fire scar and boundary",
    "text": "Map the Thomas Fire scar and boundary\n\n\nReproject CRS\n# Reproject data to match the CRS between our two datasets\nthomas_boundary= thomas_boundary.to_crs(\"EPSG:4326\")\nlandsat = landsat.rio.reproject(\"EPSG:4326\")\n\n# Confirm that the CRS of our data match\nassert landsat.rio.crs == thomas_boundary.crs\n\n\n\n\nMap the Thomas Fire\n# Initialize figure\nfig, ax = plt.subplots()\n\n# Plot the Landsat data\nlandsat[['swir22', 'nir08', 'red']].to_array().plot.imshow(ax = ax, \n                                                        robust = True)\n\n# Plot the fire perimeter\nthomas_boundary.boundary.plot(ax=ax, \n                            edgecolor='#f83c36', \n                            linewidth=2, \n                            label='Thomas Fire Boundary')\n\n# Create a legend for the false color bands and boundary\nlegend_swir = mpatches.Patch(color = \"#eb6a4b\", label = 'SWIR\\n- Burned Area')\nlegend_nir = mpatches.Patch(color = \"#a1fc81\", label = 'NIR\\n- Vegetation')\nlegend_bound = mpatches.Patch(color = \"#f83c36\", label = 'Thomas Fire\\nBoundary')\n\n# Plot legend\nax.legend(handles = [legend_swir, legend_nir, legend_bound], bbox_to_anchor=(1.38,1), fontsize = 10)\n\n# Set title and axes labels\nax.set_title('False Color Map of the 2017 Thomas Fire in California\\nwith the Fire Perimeter',\n            fontsize=14)\nax.set_xlabel('Longitude (degrees)')\nax.set_ylabel('Latitude (degrees)')\n\nplt.show()\n\n\n\nFigure Description\nThis map shows a false color image of the Thomas Fire in Santa Barbara and Ventura Counties. The fire boundary is outlined in red. Satellite data works with wavelengths of light beyond what the human eye can see. False color imagery is the process of assigning colors to these wavelengths (i.e. near-infrared and short-wave infrared). In our map, we’ve chosen to visualize short-wave infrared as red, near-infrared as green, and red wavelengths as blue. This lets us produce an image that highlights exactly where the fire scar is, as opposed to the true color image where it is much harder to distinguish. A true color image assigns the red, green, and blue wavelengths of light to the correct corresponding colors.\n\nReferences\n\nAir Quality Index (AQI) from US Environmental Protection Agency.\n\nUS Environmental Protection Agency. Air Quality System Data Mart AirNow available via https://www.epa.gov/outdoor-air-quality-data. Accessed October 17 2024.\n\nLandsat Data from Microsoft’s Planetary Computer Data Catalogue.\n\nEarth Resources Observation and Science (EROS) Center. (2020). Landsat 4-5 Thematic Mapper Level-2, Collection 2. U.S. Geological Survey. https://doi.org/10.5066/P9IAXOVV\nEarth Resources Observation and Science (EROS) Center. (2020). Landsat 7 Enhanced Thematic Mapper Plus Level-2, Collection 2. U.S. Geological Survey. https://doi.org/10.5066/P9C7I13B\nEarth Resources Observation and Science (EROS) Center. (2020). Landsat 8-9 Operational Land Imager / Thermal Infrared Sensor Level-2, Collection 2. U.S. Geological Survey. https://doi.org/10.5066/P9OGBGM6\n\nCalFire Fire Perimeter Data\n\nCalifornia Department of Forestry and Fire Protection (CAL FIRE), [calfire_all.gdb], [2024-11-17], retrieved from CAL FIRE data portal.\n\n\n\n\nAcknowledgements\nThis code is based on exercises from Carmen Galaz-Garcia for EDS-220: Working with Environmental Data."
  },
  {
    "objectID": "posts/2024-12-13-asthma-aqi/asthma-aqi.html",
    "href": "posts/2024-12-13-asthma-aqi/asthma-aqi.html",
    "title": "Wildfires and Respiratory Health in California",
    "section": "",
    "text": "The full code for this analysis can be accessed at this repository"
  },
  {
    "objectID": "posts/2024-12-13-asthma-aqi/asthma-aqi.html#introduction",
    "href": "posts/2024-12-13-asthma-aqi/asthma-aqi.html#introduction",
    "title": "Wildfires and Respiratory Health in California",
    "section": "Introduction",
    "text": "Introduction\nAsthma affects an estimated 262 million people and is the most common chronic disease among children1. It is caused by inflammation in the small airways of the lungs that leads to wheezing, coughing, and shortness of breath. While inhaled medications can manage effectively manage symptoms, if left untreated, asthma attacks can be fatal. In fact, over 450,000 people died from asthma worldwide in 2019.\nMany things can trigger asthma symptoms. One example is fine particulate matter (PM). Scientists measure PM at two scales, PM 10 and PM 2.5, where the number refers to the size of the particles in micrograms. PM 2.5 is especially concerning for people with respiratory conditions such as asthma because its small size means it can get into even the narrowest of airways, causing irritation and inflammation2.\nPM 2.5 comes from many sources, from industrial pollution to car exhaust, but recent research has shown that PM 2.5 from wildfires is particularly harmful3. As wildfire regimes become more volatile under climate change, more people will likely be exposed to these harmful particles4.\nWhile it is difficult to directly measure PM 2.5 from wildfires, one useful proxy for wildfire air pollution is air quality. The Air Quality Index (AQI) is a measure of how polluted the air is. It is calculated based on the concentrations of a number of common pollutants (Ozone, particulate matter, CO, NO2, SO2) present in the air5.\nThis analysis will use AQI data as a proxy for wildfire to explore how changing wildfire regimes are affecting the prevalence of asthma."
  },
  {
    "objectID": "posts/2024-12-13-asthma-aqi/asthma-aqi.html#data",
    "href": "posts/2024-12-13-asthma-aqi/asthma-aqi.html#data",
    "title": "Wildfires and Respiratory Health in California",
    "section": "Data",
    "text": "Data\nI will use data from a number of sources to answer this question.\n\nYearly asthma hospitalization counts by county were retrieved from the California Health and Human Services (CHHS) Open Data Portal6. This data includes the number of asthma-related hospitalizations by county and year. It also provides strata by age various age groups. I filtered down to only the observations by total population, as I am interested in hospitalizations for all age groups. Additionally, data was only available from 2015-2022, so this became my time range of interest that I filtered the rest of my data to match.\nAir Quality Index (AQI) data by county was retrieved from the EPA7. This data includes various assessments of AQI at a yearly scale across California counties. AQI is available at a finer temporal scale, but I chose to look at yearly AQI data from 2015-2022 to match the temporal scale of the asthma data.\nFire frequency and area burned data was retrieved from CalFire8. This is a geodatabase of all the fire perimeters and associated fire information from 1858 until 2023 in California. I filtered down to just fires between 2015 and 2022 and calculated the total area burned by year and the total number of fires burned by year. While this data did not factor into my main analysis, it was useful in understanding the larger context of the question and looking for confounding variables.\nCounty level demographic information and geospatial data were retrieved from the United States Census Bureau9. This data contains information on population totals by county and year for California. It also has vector data on the county boundaries. This data was used to standardize the number of hospitalizations by the population size per county and to look for trends along latitude in hospitalizations. Similarly, this data was filtered to 2015-2022.\n\nBefore beginning my analysis, I cleaned the data such that I could join all my necessary information together. I divided the raw asthma hospitalization data by the population of each county, then multiplied by 100,000 to get the hospitalizations per 100,000 people. This was to account for the varying population sizes per county. I also capped the AQI value at 500 because that is the maximum standardized value listed on the EPA’s website for air quality. I converted the county boundary polygon data into coordinates to look for trends along latitude. Lastly, I used the area data and count data from CalFire to find the total acreage burned per year and total number of fires per year. This data was used to analyze trends in wildfires over time."
  },
  {
    "objectID": "posts/2024-12-13-asthma-aqi/asthma-aqi.html#analysis-plan",
    "href": "posts/2024-12-13-asthma-aqi/asthma-aqi.html#analysis-plan",
    "title": "Wildfires and Respiratory Health in California",
    "section": "Analysis plan",
    "text": "Analysis plan\nFor my analysis, I chose a multiple linear regression model. I started by running simple linear regressions with single predictors to get a sense of what variables were and were not important to my analysis. I was interested in understanding if there was a linear relationship between wildfire PM 2.5 and asthma hospitalizations. Because I couldn’t find open access data that directly quantified PM 2.5 from wildfires, I used a variety of measures of air quality from the EPA’s Air Quality Index. I tried looking at the median AQI, maximum AQI, days with PM 2.5, days with ozone, and days unhealthy for sensitive groups or worse. While these predictors were similar, median AQI had the highest R2 and smallest p-value. There are large limitations here simply because I am using a proxy for wildfire PM 2.5 and I do not know what the relationship is between the AQI data I have and the wildfire PM 2.5 data I was unable to access.\nI also added demographic variables such as median per capita income and population density to see if these were important predictors of asthma hospitalizations. However, I found they were not significant.\nLastly, I incorporated spatial and temporal variables to see if those affected hospitalizations. There was no significant effect of latitude, but year was very significant. Thus, my final model looks as follows:\n\nMultiple linear regression model:\n\\[log(hosp\\_per\\_100k) = \\beta_0 + \\beta_1 median\\_aqi + \\beta_2year2015 ... \\beta_9year2022 + u_i\\] Where hosp_per_100k is asthma hospitalizations per 100,000 people. This variable has been log transformed to establish a linear relationship. median_aqi is the median aqi for that year. year represents the year of interest treated as a categorical variable.\nI chose to analyze year as a categorical variable because of the frequency with which the asthma data was collected. Since the asthma data is reported on a yearly basis, there is no intra-year variability in the data. Therefore, I decided to treat year as a categorical instead of a continuous numerical variable. There are 8 years of data included in my analysis, but the formula here does not list all of them for brevity.\nThe next step in my analysis plan was to check for autocorrelation because I was using time series data. Autocorrelation can occur in temporal and spatial data sets because data collected near each other in space and time have the potential to be more similar to each other. I used the autocorrelation function acf on the residuals from my linear model and there was no obvious autocorrelation in my data."
  },
  {
    "objectID": "posts/2024-12-13-asthma-aqi/asthma-aqi.html#results-and-discussion",
    "href": "posts/2024-12-13-asthma-aqi/asthma-aqi.html#results-and-discussion",
    "title": "Wildfires and Respiratory Health in California",
    "section": "Results and Discussion",
    "text": "Results and Discussion\n\nModel Results\n\n\nReveal code\nasthma_mod &lt;- lm(log(hosp_per_100k) ~ median_aqi + as.factor(year), data = asthma_aqi)\n\nasthma_hosp_table &lt;- tab_model(asthma_mod,\n                               pred.labels = c(\"Intercept (year 2015)\", \n                                               \"Median AQI\", \n                                               \"2016\",\n                                               \"2017\",\n                                               \"2018\",\n                                               \"2019\",\n                                               \"2020\",\n                                               \"2021\",\n                                               \"2022\"), \n                               dv.labels = c(\"Asthma Hospitalizations (per 100k)\"),\n                               string.ci = \"95% Conf. Int.\",\n                               string.p = \"P-value\",\n                               title = \"Table 1. Linear Model Results\",\n                               digits = 3)\n\nasthma_hosp_table\n\n\n\nTable 1. Linear Model Results\n\n\n \nAsthma Hospitalizations (per 100k)\n\n\nPredictors\nEstimates\n95% Conf. Int.\nP-value\n\n\nIntercept (year 2015)\n3.515\n3.365 – 3.666\n&lt;0.001\n\n\nMedian AQI\n0.010\n0.007 – 0.012\n&lt;0.001\n\n\n2016\n-0.411\n-0.537 – -0.286\n&lt;0.001\n\n\n2017\n-0.327\n-0.454 – -0.201\n&lt;0.001\n\n\n2018\n-0.425\n-0.548 – -0.301\n&lt;0.001\n\n\n2019\n-0.355\n-0.483 – -0.228\n&lt;0.001\n\n\n2020\n-1.138\n-1.266 – -1.009\n&lt;0.001\n\n\n2021\n-1.062\n-1.190 – -0.933\n&lt;0.001\n\n\n2022\n-0.648\n-0.778 – -0.519\n&lt;0.001\n\n\nObservations\n381\n\n\nR2 / R2 adjusted\n0.594 / 0.586\n\n\n\n\n\n\n\nThe results of the linear model show that median AQI and all years of interest had significant coefficients at the 0.05 significance level. Additionally, the adjusted R2 of 0.586 tells us that ~58.6% of the variability in hospitalizations can be explained by median aqi and year. However, if we examine the R2 values from my earlier preliminary models, we can see roughly what the breakdown of this multiple linear regression is.\n\n\n[1] \"R squared for median aqi as sole predictor: 0.07\"\n\n\n[1] \"R squared for year as sole predictor: 0.51\"\n\n\nWhen we take into account all three R2 values together, we can see that roughly 7% of the variation in the complete model comes from median AQI, and roughly 51% of the variation from year.\n\n\nModel visualization\nNext, we can plot the parts of our linear model results to see how well our predicted line fits our data.\n\n\nReveal code\nggplot(asthma_aqi, aes(median_aqi, log(hosp_per_100k))) +\n  geom_point() +\n  geom_smooth(method = \"lm\",\n              se=FALSE,\n              color=\"red\") +\n  labs(title=\"Hospitalizations and Median AQI\",\n       x=\"Median AQI\",\n       y=\"log(Hospitalizations per 100k)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nThis graph shows how hospitalizations are changing with median AQI through time. There is a weak, positive, linear relationship between median AQI and hospitalizations with no obvious outliers. We can also see our line of best fit. The residuals for this particular model were evenly distributed and showed no trend.\nWe can also visualize how hospitalizations are changing with time.\n\n\nReveal code\nggplot(asthma_aqi, aes(as.factor(year), log(hosp_per_100k))) +\n  geom_boxplot(aes(fill=as.factor(year))) +\n  theme_minimal() +\n    labs(title=\"Hospitalizations by Year\",\n       x=\"Year\",\n       y=\"log(Hospitalizations per 100k)\",\n       fill=\"Year\") \n\n\n\n\n\n\n\n\n\nThis graph shows the boxplot of hospitalizations by year. While we cannot plot our best fit line onto this graph, we can see that hospitalizations seem to be decreasing with time. This tracks with our estimated coefficients from our above multiple regression model. Every coefficient for year after the reference level of 2015 is negative, showing how hospitalizations are decreasing in magnitude relative to 2015."
  },
  {
    "objectID": "posts/2024-12-13-asthma-aqi/asthma-aqi.html#conclusions-and-next-steps",
    "href": "posts/2024-12-13-asthma-aqi/asthma-aqi.html#conclusions-and-next-steps",
    "title": "Wildfires and Respiratory Health in California",
    "section": "Conclusions and next steps",
    "text": "Conclusions and next steps\nOverall, these results only provide very weak evidence that there is a relationship between wildfires and respiratory health. While the results of my model were significant, there were many limitations in the data I was using that could be affecting my results. For example, a large limitation of this analysis was the lack of finer temporal scale data for asthma-related hospitalizations. All of the other datasets I was using had data available at the daily and sometimes even hourly time scale. Because I had to aggregate data about AQI to such a broad time scale, much of the variation in AQI across seasons is lost. There are certain asthma triggers that are seasonal, such as pollen in the spring or cold air in the winter. Being able to track how month-to-month seasonality is affecting hospitalizations would be a great next step.\nAnother limitation was that I was unable to find any open-source data on PM 2.5 directly from wildfires. This meant I was unable to examine a direct relationship between wildfire and asthma hospitalizations, I could only use AQI as a proxy. There certainly is data on wildfire PM 2.5, and if given more time, I would reach out to the data owners and authors of papers who have published scholarship on this topic to try and request access to the data.\nLastly, this analysis contained many omitted variables that might be influencing trends in asthma hospitalizations over time.\nFor example:\n\n\nFires and hospitalizations over time\n# Plot change in area burned over time\nfire_plot &lt;- ggplot(fire_asthma, aes(year, total_area)) +\n  geom_col(fill=\"firebrick\") + \n  geom_col(data = . %&gt;% filter(year == 2020),\n           color=\"red\",\n           fill=\"firebrick\",\n           lwd=1) +\n  labs(title=\"Total Area Burned by Wildfires\\nin California by Year\",\n       x=\"Year\",\n       y=\"Area burned (acres)\") +\n  theme_minimal() +\n    theme(axis.title = element_text(size=12),\n        title = element_text(size=12))\n\n# Plot change in hospitalizations over time\nhosp_plot &lt;- ggplot(fire_asthma, aes(year, total_hosp_capita)) +\n  geom_col(fill=\"cornflowerblue\") +\n  geom_col(data = . %&gt;% filter(year == 2020),\n           color=\"red\",\n           fill=\"cornflowerblue\",\n           lwd=1) +\n  labs(title=\"Total Asthma Hospitalizations\\nper 100k in California by Year\",\n       x=\"Year\",\n       y=\"Hospitalizations per 100K\") +\n  theme_minimal() +\n  theme(axis.title = element_text(size=12),\n        title = element_text(size=12))\n\nfire_plot | hosp_plot  \n\n\n\n\n\n\n\n\n\nThis plot shows the total area burned by wildfires in California from 2015 to 2022, and the total number of asthma hospitalizations per 100k people in California from 2015-2022. The year 2020 in particular stands out. Despite it being a record year for the amount of area burned in the state, it was also the year with the fewest hospitalizations. An obvious factor that is likely influencing this is COVID-19. The shelter-in-place mandates likely greatly influenced people who were seeking out asthma care, even if there were more wildfires that year.\nThis is just one example of an important cause of variation in asthma hospitalizations that I was unable to capture in my model. Going forward, I would like to have access to much more comprehensive data to truly unpack the connection between wildfires and respiratory health.\n\nReferences\n\nAcknowledgements\nThis code is based on material from Max Czapanskiy for EDS-222: Statistics for Environmental Data Scientists."
  },
  {
    "objectID": "posts/2024-12-13-asthma-aqi/asthma-aqi.html#footnotes",
    "href": "posts/2024-12-13-asthma-aqi/asthma-aqi.html#footnotes",
    "title": "Wildfires and Respiratory Health in California",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWorld Health Organization: WHO, World Health Organization: WHO. Asthma. 6 May 2024. Available: https://www.who.int/news-room/fact-sheets/detail/asthma↩︎\nHealth studies of criteria air pollutants. In: California Office of Environmental Health Hazard Assessment [Internet]. [cited 12 Dec 2024]. Available: https://oehha.ca.gov/air/health-studies-criteria-air-pollutants↩︎\nAguilera, R., Corringham, T., Gershunov, A. et al. Wildfire smoke impacts respiratory health more than fine particles from other sources: observational evidence from Southern California. Nat Commun 12, 1493 (2021). https://doi.org/10.1038/s41467-021-21708-0↩︎\nD. Kiser et al., “Particulate matter and emergency visits for asthma: a time-series study of their association in the presence and absence of wildfire smoke in Reno, Nevada, 2013–2018,” Environ. Health, vol. 19, no. 1, p. 92, Aug. 2020, doi: 10.1186/s12940-020-00646-2.↩︎\nTechnical assistance document for the reporting of daily air quality – the Air Quality Index (AQI). United States Environmental Protection Agency. 2024 May. Report No.: EPA-454/B-24-002. Available: https://document.airnow.gov/technical-assistance-document-for-the-reporting-of-daily-air-quailty.pdf↩︎\n“Asthma Emergency Department Visit Rates by County - California Health and Human Services Open Data Portal,” CHHS Open Data. https://data.ca.gov/dataset/asthma-emergency-department-visit-rates (accessed Dec. 02, 2023).↩︎\nUS Environmental Protection Agency. Air Quality System Data Mart AirNow available via https://www.epa.gov/outdoor-air-quality-data. Accessed December 8 2024.↩︎\nCalifornia Department of Forestry and Fire Protection (CAL FIRE), [calfire_all.gdb], [2024-11-17], retrieved from CAL FIRE data portal. Accessed December 8 2024.↩︎\nUnited States Census Bureau. County Population Totals: 2010-2019, 2020-2023. https://www.census.gov/data/datasets/time-series/demo/popest/2010s-counties-total.html#par_textimage_70769902. Accessed December 8 2024.↩︎"
  },
  {
    "objectID": "posts/2025-03-21-rainfall/rainfall.html",
    "href": "posts/2025-03-21-rainfall/rainfall.html",
    "title": "Reproducing the Weather",
    "section": "",
    "text": "the Weather When You Were Born Might Affect How Wealthy You Are as an Adult? \nWell, if you are an Indonesian woman born in a rural area between 1953-1974, that just might be the case.\nIn 2009, Dr. Sharon Maccini and Dr. Dean Yang published a seminal paper investigating the effects of early-life rainfall shocks on life outcomes titled “Under the Weather: Health, Schooling, and Economic Consequences of Early-Life Rainfall”[1]. Interestingly, they found that higher early-life rainfall leads to improved health, schooling, and socioeconomic status for women. They suggest that more rainfall leads to more agricultural output in rural areas, which means increased early-life nutrition and overall greater life outcomes.\nThey also found none of the same results for men, suggesting they were resistant to any effects of rainfall shock. This supports existing research about gender bias and the idea that nutrition and resources are preferentially allocated to men during times of hardship [2].\nAs part of my study of causal inference and econometric statistical techniques in Dr. Adam Garber’s course, I set out to replicate the main findings of this study.\nThe data included were:\n\nIndonesian Family Life Surveys (IFLS): Birth year, location, health status, education, and socioeconomic status for 4615 women and 4277 men born between 1953-1974 and sampled until 2000.\nRainfall Data: From 1953-1995 Global Historical Climatology Network Precipitation and Temperature Data across 378 measurement stations.\n\nHowever, the rainfall data was notably missing from the author’s data repository. While we did our best to locate the original data from NOAA, we were unable to find it. So, our professor very kindly simulated the entire dataset for this project. Thank you, Adam! Because this data is simulated, the resulting statistics will not be realistic. That being said, the aim is to replicate the general trend of the findings in the original study.\n\n\nThis study employs two main specification strategies as part of their causal inference: instrumental variable regression and fixed effects.\n \\[ Y_i{}_j{}_s{}_t = \\beta R_j{}_t + \\mu_j{}_s + \\delta_s{}_t + \\epsilon_i{}_j{}_s{}_t\\]  Where\n\nYijst: Adult outcome of adult i born in district j, in season s and in year t\nβ: Impact of (instrumented) birth year rainfall Rjt on the adult outcome\nμjs: Fixed effect for individuals born in district j and season s\nδst: Fixed effect for the birth year t and season s combination\nεijst: Mean-zero error term\n\n\n\nThe main predictor variable for this study is birth year rainfall in one’s birth district. However, the authors note that measurement error is a large concern for the rainfall stations. Even minute variations in the performance of the equipment from station to station can magnify into large differences in final results. Thus, they instrument rainfall in one’s birth district with rainfall in the 2nd-5th next closest districts.\nInstrumental variables are useful when one of the predictor variables is endogenous, or correlated with the error term of the model. While this is often the case when a study has omitted variable bias or simultaneity, it can also be caused by measurement error. While this last option is not the most ubiquitous use of instrumental variables, it is still a novel and effective way to account for endogenous predictors.\n\n\n\nThe authors also employ fixed effects for the combination of birthyear and season, and district and season. Fixed effects are a way to account for fixed differences between groups or strata of your data that you don’t want to bias your final causal results. For example, if the topography of district A means farming is more challenging there than in district B, you don’t want that district-specific difference to influence any conclusions you might draw about life oucomes for individuals in district A compared to district B. This helps us create a more valid counterfactual and isolate only the effect of rainfall shock on life outcomes.\nSeason is a crucial variable for this study because Indonesia has two main seasons: wet and dry. As the names suggest, these correlate to seasons of low rainfall and high rainfall, so it’s important we control for constant differences in rainfall across seasons. The authors look at the interaction of season with both district and year to account for the semi-constant spatial and temporal variation across their study area.\n\n\n\n\n\n\nLoad libraries\nlibrary(tidyverse)\nlibrary(here)\nlibrary(AER)\nlibrary(kableExtra)\nlibrary(fixest)\nlibrary(gt)\nlibrary(jtools)\nlibrary(modelsummary)\nlibrary(stargazer)\nlibrary(sandwich)\nlibrary(lmtest)\nlibrary(readr)\nlibrary(lfe)    \n\n\n\n# Load simulated rain data\nrainfall &lt;- read_csv(here(\"posts\",\"2025-03-21-rainfall\", \"data\", \"Simulated_Data_MakeItRain.csv\"))\n\nHere, I’m create the fixed effects variables using paste0 which will be used later in our model construction.\n\n# Create fixed effect levels \nrainfall &lt;- rainfall %&gt;%\n  mutate(district_season = as.factor(paste0(district_id, \"_\", season)),\n         birthyear_season = as.factor(paste0(birth_year, \"_\", season)))\n\nThe study is interested in differential outcomes between men and women, so we split the dataset into two.\n\n# Split data to men and women\nfemale_data &lt;- rainfall %&gt;% filter(female == 1)\nmale_data &lt;- rainfall %&gt;% \n  filter(female == 0) %&gt;% \n  # Male data had no variation in outcomes, so I simulate it here\n  # I assumed all outcomes would be slightly higher for men because of the gender gap\n  mutate(height = rnorm(n = 2023, mean = 162, sd = 0.5),\n         education = rnorm(n = 2023, mean = 7.8, sd = 0.5),\n         asset_index = rnorm(n = 2023, mean = 0.2, sd = 0.2),\n         health_poor = rnorm(n = 2023, mean = 0.15, 0.1))\n\n\n\nNow that our data is split, we can run our FE IV model!\nThe main predictive variable of this study is the deviation between log rainfall in one’s birth district and year from the log average rainfall in one’s birth district from 1953-1999. This transformation has already been completed, so we can plug our rainfall measurements for the 5 rainfall stations directly into our model.\nI will be using the feols() function from the fixest package. While this function is primarily designed around incorporating fixed effects into standard ols regression, it can also handle instrumental variables. We start by listing our outcome variables of interest c(height, education, asset_index, health_poor) as a function of 1, which is a place holder for our instrumented variable rain_closest. Next, we give our fixed effects of district_season and birthyear_season. And lastly, our instrumental variables, which we specify as rain_closest instrumented by rain_2nd, rain_3rd, rain_4th, and rain_5th. We also cluster standard errors by province, as they do in the original study.\n\n# Regression analysis for women\nfeols_female &lt;- feols(c(height, education, asset_index, health_poor) ~ 1 | # outcomes\n                        district_season + birthyear_season | # FEs\n                        rain_closest ~ rain_2nd + rain_3rd + # IVs\n                        rain_4th + rain_5th,\n                    cluster = ~ province, \n                    data = female_data)\n\n\n\nReveal code\n# Create table output as a kable object\ntable_female &lt;- modelsummary(\n  feols_female,\n  stars = TRUE, fmt = 3, \n  coef_map = c(\"fit_rain_closest\" = \"Rainfall deviation(log)\"),\n  gof_omit = 'DF|Deviance|R2 Within|R2 Within Adj.|AIC|BIC|RMSE',\n  title = \"Effects of birthyear rainfall on women's adult life outcomes\",\n  col.names = NULL, # Column names weren't formatting correctly\n  output = \"kableExtra\"\n)\n\n# Manually rename the column headers\ntable_female &lt;- table_female %&gt;%\n  kableExtra::add_header_above(\n    c(\" \" = 1, \"Height\" = 1, \"Education\" = 1, \"Asset Index\" = 1, \"Health (Poor)\" = 1)\n  ) %&gt;% \n  kable_styling(font_size = 18)\n\ntable_female\n\n\n\n\n\nHeight\nEducation\nAsset Index\nHealth (Poor)\n\nEffects of birthyear rainfall on women's adult life outcomes\n\n  \n    Rainfall deviation(log) \n    2.850*** \n    1.100*** \n    0.600*** \n    −0.190*** \n  \n  \n     \n    (0.000) \n    (0.000) \n    (0.000) \n    (0.000) \n  \n  \n    Num.Obs. \n    1977 \n    1977 \n    1977 \n    1977 \n  \n  \n    R2 \n    1.000 \n    1.000 \n    1.000 \n    1.000 \n  \n  \n    R2 Adj. \n    1.000 \n    1.000 \n    1.000 \n    1.000 \n  \n  \n    Std.Errors \n    by: province \n    by: province \n    by: province \n    by: province \n  \n  \n    FE: district_season \n    X \n    X \n    X \n    X \n  \n  \n    FE: birthyear_season \n    X \n    X \n    X \n    X \n  \n\n\n + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n\n\n\n\n\n# Regression analysis for men\nfeols_male &lt;- feols(c(height, education, asset_index, health_poor) ~ 1 | # outcomes\n                      district_season + birthyear_season | # FEs\n                      rain_closest ~ rain_2nd + rain_3rd + # IVs\n                        rain_4th + rain_5th,\n                    cluster = ~ province, \n                    data = male_data)\n\n\n\nReveal code\n# Create table output as a kable object\ntable_male &lt;- modelsummary(\n  feols_male,\n  stars = TRUE, fmt = 3, \n  coef_map = c(\"fit_rain_closest\" = \"Rainfall deviation(log)\"),\n  gof_omit = 'DF|Deviance|R2 Within|R2 Within Adj.|AIC|BIC|RMSE',\n  title = \"Effects of birthyear rainfall on men's adult life outcomes\",\n  col.names = NULL, # Column names weren't formatting correctly\n  output = \"kableExtra\"\n)\n\n# Manually rename the column headers\ntable_male &lt;- table_male %&gt;%\n  kableExtra::add_header_above(\n    c(\" \" = 1, \"Height\" = 1, \"Education\" = 1, \"Asset Index\" = 1, \"Health (Poor)\" = 1)\n  ) %&gt;% \n  kable_styling(font_size = 18)\n\ntable_male\n\n\n\n\n\nHeight\nEducation\nAsset Index\nHealth (Poor)\n\nEffects of birthyear rainfall on men's adult life outcomes\n\n  \n    Rainfall deviation(log) \n    −0.306* \n    0.043 \n    0.014 \n    0.014 \n  \n  \n     \n    (0.107) \n    (0.136) \n    (0.040) \n    (0.018) \n  \n  \n    Num.Obs. \n    2023 \n    2023 \n    2023 \n    2023 \n  \n  \n    R2 \n    0.128 \n    0.121 \n    0.147 \n    0.131 \n  \n  \n    R2 Adj. \n    −0.012 \n    −0.020 \n    0.011 \n    −0.008 \n  \n  \n    Std.Errors \n    by: province \n    by: province \n    by: province \n    by: province \n  \n  \n    FE: district_season \n    X \n    X \n    X \n    X \n  \n  \n    FE: birthyear_season \n    X \n    X \n    X \n    X \n  \n\n\n + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n\n\n\n\n\n\n\n\nOverall, we see similar life outcome trends as Maccini and Yang did in their study. An increase in birthyear rainfall had a significant effect in increasing women’s height, education levels, and asset index while decreasing their likelihood to identify as poor health. Alternatively, we see none of these trends for men.\nAdditionally, Maccini and Yang note that the causal chain connecting rainfall and life outcomes is something like early-life rainfall to infant health, to educational attainment, and finally to adult socioeconomic status.\nThis study shows the importance of supporting girls in their early life stages because the consequences can ripple throughout their entire lifetime."
  },
  {
    "objectID": "posts/2025-03-21-rainfall/rainfall.html#specification-strategy",
    "href": "posts/2025-03-21-rainfall/rainfall.html#specification-strategy",
    "title": "Reproducing the Weather",
    "section": "",
    "text": "This study employs two main specification strategies as part of their causal inference: instrumental variable regression and fixed effects.\n \\[ Y_i{}_j{}_s{}_t = \\beta R_j{}_t + \\mu_j{}_s + \\delta_s{}_t + \\epsilon_i{}_j{}_s{}_t\\]  Where\n\nYijst: Adult outcome of adult i born in district j, in season s and in year t\nβ: Impact of (instrumented) birth year rainfall Rjt on the adult outcome\nμjs: Fixed effect for individuals born in district j and season s\nδst: Fixed effect for the birth year t and season s combination\nεijst: Mean-zero error term\n\n\n\nThe main predictor variable for this study is birth year rainfall in one’s birth district. However, the authors note that measurement error is a large concern for the rainfall stations. Even minute variations in the performance of the equipment from station to station can magnify into large differences in final results. Thus, they instrument rainfall in one’s birth district with rainfall in the 2nd-5th next closest districts.\nInstrumental variables are useful when one of the predictor variables is endogenous, or correlated with the error term of the model. While this is often the case when a study has omitted variable bias or simultaneity, it can also be caused by measurement error. While this last option is not the most ubiquitous use of instrumental variables, it is still a novel and effective way to account for endogenous predictors.\n\n\n\nThe authors also employ fixed effects for the combination of birthyear and season, and district and season. Fixed effects are a way to account for fixed differences between groups or strata of your data that you don’t want to bias your final causal results. For example, if the topography of district A means farming is more challenging there than in district B, you don’t want that district-specific difference to influence any conclusions you might draw about life oucomes for individuals in district A compared to district B. This helps us create a more valid counterfactual and isolate only the effect of rainfall shock on life outcomes.\nSeason is a crucial variable for this study because Indonesia has two main seasons: wet and dry. As the names suggest, these correlate to seasons of low rainfall and high rainfall, so it’s important we control for constant differences in rainfall across seasons. The authors look at the interaction of season with both district and year to account for the semi-constant spatial and temporal variation across their study area."
  },
  {
    "objectID": "posts/2025-03-21-rainfall/rainfall.html#study-replication",
    "href": "posts/2025-03-21-rainfall/rainfall.html#study-replication",
    "title": "Reproducing the Weather",
    "section": "",
    "text": "Load libraries\nlibrary(tidyverse)\nlibrary(here)\nlibrary(AER)\nlibrary(kableExtra)\nlibrary(fixest)\nlibrary(gt)\nlibrary(jtools)\nlibrary(modelsummary)\nlibrary(stargazer)\nlibrary(sandwich)\nlibrary(lmtest)\nlibrary(readr)\nlibrary(lfe)    \n\n\n\n# Load simulated rain data\nrainfall &lt;- read_csv(here(\"posts\",\"2025-03-21-rainfall\", \"data\", \"Simulated_Data_MakeItRain.csv\"))\n\nHere, I’m create the fixed effects variables using paste0 which will be used later in our model construction.\n\n# Create fixed effect levels \nrainfall &lt;- rainfall %&gt;%\n  mutate(district_season = as.factor(paste0(district_id, \"_\", season)),\n         birthyear_season = as.factor(paste0(birth_year, \"_\", season)))\n\nThe study is interested in differential outcomes between men and women, so we split the dataset into two.\n\n# Split data to men and women\nfemale_data &lt;- rainfall %&gt;% filter(female == 1)\nmale_data &lt;- rainfall %&gt;% \n  filter(female == 0) %&gt;% \n  # Male data had no variation in outcomes, so I simulate it here\n  # I assumed all outcomes would be slightly higher for men because of the gender gap\n  mutate(height = rnorm(n = 2023, mean = 162, sd = 0.5),\n         education = rnorm(n = 2023, mean = 7.8, sd = 0.5),\n         asset_index = rnorm(n = 2023, mean = 0.2, sd = 0.2),\n         health_poor = rnorm(n = 2023, mean = 0.15, 0.1))\n\n\n\nNow that our data is split, we can run our FE IV model!\nThe main predictive variable of this study is the deviation between log rainfall in one’s birth district and year from the log average rainfall in one’s birth district from 1953-1999. This transformation has already been completed, so we can plug our rainfall measurements for the 5 rainfall stations directly into our model.\nI will be using the feols() function from the fixest package. While this function is primarily designed around incorporating fixed effects into standard ols regression, it can also handle instrumental variables. We start by listing our outcome variables of interest c(height, education, asset_index, health_poor) as a function of 1, which is a place holder for our instrumented variable rain_closest. Next, we give our fixed effects of district_season and birthyear_season. And lastly, our instrumental variables, which we specify as rain_closest instrumented by rain_2nd, rain_3rd, rain_4th, and rain_5th. We also cluster standard errors by province, as they do in the original study.\n\n# Regression analysis for women\nfeols_female &lt;- feols(c(height, education, asset_index, health_poor) ~ 1 | # outcomes\n                        district_season + birthyear_season | # FEs\n                        rain_closest ~ rain_2nd + rain_3rd + # IVs\n                        rain_4th + rain_5th,\n                    cluster = ~ province, \n                    data = female_data)\n\n\n\nReveal code\n# Create table output as a kable object\ntable_female &lt;- modelsummary(\n  feols_female,\n  stars = TRUE, fmt = 3, \n  coef_map = c(\"fit_rain_closest\" = \"Rainfall deviation(log)\"),\n  gof_omit = 'DF|Deviance|R2 Within|R2 Within Adj.|AIC|BIC|RMSE',\n  title = \"Effects of birthyear rainfall on women's adult life outcomes\",\n  col.names = NULL, # Column names weren't formatting correctly\n  output = \"kableExtra\"\n)\n\n# Manually rename the column headers\ntable_female &lt;- table_female %&gt;%\n  kableExtra::add_header_above(\n    c(\" \" = 1, \"Height\" = 1, \"Education\" = 1, \"Asset Index\" = 1, \"Health (Poor)\" = 1)\n  ) %&gt;% \n  kable_styling(font_size = 18)\n\ntable_female\n\n\n\n\n\nHeight\nEducation\nAsset Index\nHealth (Poor)\n\nEffects of birthyear rainfall on women's adult life outcomes\n\n  \n    Rainfall deviation(log) \n    2.850*** \n    1.100*** \n    0.600*** \n    −0.190*** \n  \n  \n     \n    (0.000) \n    (0.000) \n    (0.000) \n    (0.000) \n  \n  \n    Num.Obs. \n    1977 \n    1977 \n    1977 \n    1977 \n  \n  \n    R2 \n    1.000 \n    1.000 \n    1.000 \n    1.000 \n  \n  \n    R2 Adj. \n    1.000 \n    1.000 \n    1.000 \n    1.000 \n  \n  \n    Std.Errors \n    by: province \n    by: province \n    by: province \n    by: province \n  \n  \n    FE: district_season \n    X \n    X \n    X \n    X \n  \n  \n    FE: birthyear_season \n    X \n    X \n    X \n    X \n  \n\n\n + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n\n\n\n\n\n# Regression analysis for men\nfeols_male &lt;- feols(c(height, education, asset_index, health_poor) ~ 1 | # outcomes\n                      district_season + birthyear_season | # FEs\n                      rain_closest ~ rain_2nd + rain_3rd + # IVs\n                        rain_4th + rain_5th,\n                    cluster = ~ province, \n                    data = male_data)\n\n\n\nReveal code\n# Create table output as a kable object\ntable_male &lt;- modelsummary(\n  feols_male,\n  stars = TRUE, fmt = 3, \n  coef_map = c(\"fit_rain_closest\" = \"Rainfall deviation(log)\"),\n  gof_omit = 'DF|Deviance|R2 Within|R2 Within Adj.|AIC|BIC|RMSE',\n  title = \"Effects of birthyear rainfall on men's adult life outcomes\",\n  col.names = NULL, # Column names weren't formatting correctly\n  output = \"kableExtra\"\n)\n\n# Manually rename the column headers\ntable_male &lt;- table_male %&gt;%\n  kableExtra::add_header_above(\n    c(\" \" = 1, \"Height\" = 1, \"Education\" = 1, \"Asset Index\" = 1, \"Health (Poor)\" = 1)\n  ) %&gt;% \n  kable_styling(font_size = 18)\n\ntable_male\n\n\n\n\n\nHeight\nEducation\nAsset Index\nHealth (Poor)\n\nEffects of birthyear rainfall on men's adult life outcomes\n\n  \n    Rainfall deviation(log) \n    −0.306* \n    0.043 \n    0.014 \n    0.014 \n  \n  \n     \n    (0.107) \n    (0.136) \n    (0.040) \n    (0.018) \n  \n  \n    Num.Obs. \n    2023 \n    2023 \n    2023 \n    2023 \n  \n  \n    R2 \n    0.128 \n    0.121 \n    0.147 \n    0.131 \n  \n  \n    R2 Adj. \n    −0.012 \n    −0.020 \n    0.011 \n    −0.008 \n  \n  \n    Std.Errors \n    by: province \n    by: province \n    by: province \n    by: province \n  \n  \n    FE: district_season \n    X \n    X \n    X \n    X \n  \n  \n    FE: birthyear_season \n    X \n    X \n    X \n    X \n  \n\n\n + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001"
  },
  {
    "objectID": "posts/2025-03-21-rainfall/rainfall.html#results",
    "href": "posts/2025-03-21-rainfall/rainfall.html#results",
    "title": "Reproducing the Weather",
    "section": "",
    "text": "Overall, we see similar life outcome trends as Maccini and Yang did in their study. An increase in birthyear rainfall had a significant effect in increasing women’s height, education levels, and asset index while decreasing their likelihood to identify as poor health. Alternatively, we see none of these trends for men.\nAdditionally, Maccini and Yang note that the causal chain connecting rainfall and life outcomes is something like early-life rainfall to infant health, to educational attainment, and finally to adult socioeconomic status.\nThis study shows the importance of supporting girls in their early life stages because the consequences can ripple throughout their entire lifetime."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Haylee Oyler",
    "section": "",
    "text": "I’ve always been interested in understanding the intersection of people and nature. For me, the most conspicuous path into this nexus was a degree in environmental science. My time at UC Berkeley gave me experience studying human-environment interactions. It also introduced me to working with data. My senior thesis used large datasets from the birdwatching platform eBird to document how community science participation changed during the 2020 pandemic. I found the power and scope of community-centered data captivating and wanted to dive deeper into questions of access, equity, and representation in data collection.\nNow, I am a Master of Environmental Data Science student at UC Santa Barbara. I’m excited about continuing to apply data science tools to study issues of environmental justice and community-based conservation. My approach to solving environmental problems centers first and foremost on people, particularly the participation and experience of historically marginalized groups."
  },
  {
    "objectID": "about.html#about",
    "href": "about.html#about",
    "title": "Haylee Oyler",
    "section": "",
    "text": "I’ve always been interested in understanding the intersection of people and nature. For me, the most conspicuous path into this nexus was a degree in environmental science. My time at UC Berkeley gave me experience studying human-environment interactions. It also introduced me to working with data. My senior thesis used large datasets from the birdwatching platform eBird to document how community science participation changed during the 2020 pandemic. I found the power and scope of community-centered data captivating and wanted to dive deeper into questions of access, equity, and representation in data collection.\nNow, I am a Master of Environmental Data Science student at UC Santa Barbara. I’m excited about continuing to apply data science tools to study issues of environmental justice and community-based conservation. My approach to solving environmental problems centers first and foremost on people, particularly the participation and experience of historically marginalized groups."
  },
  {
    "objectID": "about.html#background",
    "href": "about.html#background",
    "title": "Haylee Oyler",
    "section": "Background",
    "text": "Background\n\n\n\nGrowing up in California’s Central Valley, I saw the extremes of environmental health. I was fortunate enough to have access to natural wonders like Yosemite Valley, Monterey Bay, and the Mojave Desert. And yet, my city has some of the poorest air quality in the nation due, in part, to the prolific extractive petroleum industry in the region. These formative years meant I came to appreciate the beauty and wonder of nature while also realizing how human activity was creating unprecedented environmental change.\n\n\nI brought this foundation with me to my studies at UC Berkeley. Studying at the Rausser College of Natural Resources gave me the breadth and depth of knowledge to contextualize these experiences. The interdisciplinary nature of the degree also allowed me the freedom to explore what I was most interested in."
  },
  {
    "objectID": "about.html#research",
    "href": "about.html#research",
    "title": "Haylee Oyler",
    "section": "Research",
    "text": "Research\n\n\n\nI’ve conducted research across a variety of environmental topics–everything from microplastics to wildfire to plant competition. My path through science hasn’t been linear, but this nonlinearity has helped me realize that the work I am most passionate about is interdisciplinary. The one through-line across my studies has been people.\n\n\nWhen I learned about science growing up, I learned that science is objective, that it is a way of knowing that yields truth. When I started to do science, however, I quickly realized that was not the case. Systems are messy and multifaceted and the ways we have to quantify them are imperfect. As I grew, I couldn’t help but situate the lens of my environmental studies from the perspective of the people most impacted by my question. I want to understand the effectiveness of a marine protected area because I want to know if it will improve the water quality and catch for the fishing village. I want to understand patterns in bird diversity observations because I want to know who chooses to participate in community science and why.\n\n\nNow, I broadly define my research interests around socio-ecological systems, or systems that include the social aspects of human culture, politics, economy, and technology and how they interact with natural systems. As I grew into myself and learned from wonderful mentors, I decided to embrace the fuzziness around the edges of real science. Truth often resists simplicity. My ethos for research centers around leaning into that complexity. This means incorporating methods across disciplines such as sociology, public health, and critical studies to examine environmental questions."
  },
  {
    "objectID": "about.html#leisure",
    "href": "about.html#leisure",
    "title": "Haylee Oyler",
    "section": "Leisure",
    "text": "Leisure\n\n\nIn my spare time, I enjoy rock climbing. Indoor, outdoor, bouldering, sport climbing–I love it all. I’m always looking for more cool places to climb and cool people to climb with. The last few spots I’ve been to are Santa Barbara’s The Brickyard, Joshua Tree’s Jumbo Rocks, and Bishop’s Owen’s River Gorge. Check out photos from some of my rocky adventures under the resources tab.\nI also enjoy a number of artistic hobbies like reading, writing, music, and drawing–all with varying degrees of novice proficiency. I love to try new things, even if it means sitting in the discomfort of the learning curve for a little bit. While science has become the focus of my career, I am a thorough appreciator of art in all forms. (And frankly, I think there should be more art in science!)\nAnd lastly, I appreciate a good bird! You can find a selection of some of the feathered friends I’ve managed to capture with my lens under the photos page."
  },
  {
    "objectID": "photos.html",
    "href": "photos.html",
    "title": "Photo Gallery",
    "section": "",
    "text": "Here is a collection of photos of some of my favorite things. Work, school, fun, it’s all here!"
  },
  {
    "objectID": "photos.html#sedgwick-uc-reserve",
    "href": "photos.html#sedgwick-uc-reserve",
    "title": "Photo Gallery",
    "section": "Sedgwick UC Reserve",
    "text": "Sedgwick UC Reserve\nMy previous job was with Dr. Nathan Kraft’s plant ecology lab at UCLA. A large component of the job was field work at the Sedgwick UC Reserve in the Santa Ynez Valley. The lab has an experimental plot where conduct research related to competition and coexistence with a variety of native and invasive California annual plants. You can see some photos of the plot being prepared for each year’s experimental design below.\n\n\n\n\n\n\n\n\n1 / 5\n\n\n\nLaying down the matting\n\n\n\n\n\n2 / 5\n\n\nPreparing the plot\n\n\n\n\n\n3 / 5\n\n\n\n\n\n\n\n\n4 / 5\n\n\n\n\n\n\n\n\n5 / 5\n\n\nAcmispon wrangelianus and Salvia columbariae\n\n\n\n\n\n❮\n\n\n❯\n\n\n\nAs with most field work, weather was a major factor for the research we could conduct. Several “atmospheric rivers” meant the plot was often soggy and wet. We even lost a year’s worth of plants to a series of particularly bad storms. We learned from those mistakes, however, and came back the next year better prepared with straw buffers and strategically placed drainage trenches.\nUnfortunately, water was not the only thing we had to worry about. Much of the Sedgwick Reserve burned in the 2024 Lake Fire. The plot was saved thanks to the tireless work of fire crews, but the entire surrounding hillside was burned. Despite the challenges of extreme weather, our lab was still able to complete the experiments successfully. Publication of our results is forthcoming in 2025."
  },
  {
    "objectID": "photos.html#hopland-research-and-extension-center",
    "href": "photos.html#hopland-research-and-extension-center",
    "title": "Photo Gallery",
    "section": "Hopland Research and Extension Center",
    "text": "Hopland Research and Extension Center\nI spent some time working at the Hopland Research and Extension Center with Dr. Kendall Calhoun from UC Berkeley. We were studying wildlife response to wildfire in the area. Our main data collection came from camera traps and audio monitors we placed across the reserve. Most of our fieldwork involved setting and collecting the traps every few days.\n\n\n\n\n\n\n\n\n\n1 / 5\n\n\n\n\n\n\n\n\n2 / 5\n\n\n\n\n\n\n\n\n3 / 5\n\n\n\n\n\n\n\n\n4 / 5\n\n\n\n\n\n\n\n\n5 / 5\n\n\n\n\n\n\n\n\n❮\n\n\n❯\n\n\n\nAgain, Hopland is another area that has felt the effects of extreme weather in California. The 2018 Mendocino Complex Fire burned about half of the reserve. While devastating, it created a unique opportunity to study how wildlife occupancy was changing in the burned and unburned sections. We were able to examine how fire severity and pyrodiversity where affecting avian and bat species in the region, with moderate severity fire increasing the occupancy of bats, insectivorous birds, and tree nesting birds, while high severity fire decreased occupancy of ground nesting birds and granivorous birds."
  },
  {
    "objectID": "photos.html#monteverde-research-institue",
    "href": "photos.html#monteverde-research-institue",
    "title": "Photo Gallery",
    "section": "Monteverde Research Institue",
    "text": "Monteverde Research Institue\nWhile at Berkeley, I participated in a study abroad program to Costa Rica. We studied tropical biology and conservation through a combination of coursework, field experiences, and independent research. While there were many wonderful memories made over the several months I was there, the research project was certainly the highlight of the program.\n\n\n\n\n\n\n\n\n\n1 / 5\n\n\n\n\n\n\n\n\n2 / 5\n\n\nCollecting sediment samples to look for microplastics\n\n\n\n\n\n3 / 5\n\n\nCuajiniquil’s mangroves\n\n\n\n\n\n4 / 5\n\n\nLocal aquaculture in Cuajiniquil\n\n\n\n\n\n5 / 5\n\n\n\n\n\n\n\n\n❮\n\n\n❯\n\n\n\nI chose to study how microplastics were changing in waterways along a gradient of human influence. This involved a homestay in the fishing village of Cuajiniquil, a town on the Northwest coast of Costa Rica. I collected sediment samples from the banks of three different rivers: one that ran through the town, one that ran through farmland, and one that ran through a protected area. Unsurpisingly, microplastic presence increased with proximity to humans."
  },
  {
    "objectID": "photos.html#climbing-adventures",
    "href": "photos.html#climbing-adventures",
    "title": "Photo Gallery",
    "section": "Climbing Adventures",
    "text": "Climbing Adventures\n\n\n\n\n\n\n\n\n\n1 / 5\n\n\nJoshua Tree National Park\n\n\n\n\n\n2 / 5\n\n\n\n\n\n\n\n\n3 / 5\n\n\n\n\n\n\n\n\n4 / 5\n\n\nEcho Cliffs, Santa Monica Mountains\n\n\n\n\n\n5 / 5\n\n\n\n\n\n\n\n\n❮\n\n\n❯"
  },
  {
    "objectID": "photos.html#birds",
    "href": "photos.html#birds",
    "title": "Photo Gallery",
    "section": "Birds!",
    "text": "Birds!\n\n\n\n\n\n\n\n\n\n1 / 5\n\n\nBlack Phoebe\n\n\n\n\n\n2 / 5\n\n\nAllen’s Hummingbird\n\n\n\n\n\n3 / 5\n\n\nDark-eyed Junco\n\n\n\n\n\n4 / 5\n\n\nGreat Egret\n\n\n\n\n\n5 / 5\n\n\nCalifornia Quail\n\n\n\n\n\n❮\n\n\n❯"
  }
]